{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN and GAE framework for RL agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, xmin=0, xmax=100, ymin=0, ymax=100, num_rectangles=5, num_circles=5, min_distance=50):\n",
    "        self.xlim = (xmin, xmax)\n",
    "        self.ylim = (ymin, ymax)\n",
    "        self.num_rectangles = num_rectangles\n",
    "        self.num_circles = num_circles\n",
    "        self.min_distance = min_distance\n",
    "        self.random_obstacles()\n",
    "        self.x_start, self.y_start = self.random_start()\n",
    "        self.x_goal, self.y_goal = self.random_goal()\n",
    "\n",
    "    def random_obstacles(self):\n",
    "        # Generate random rectangles\n",
    "        self.obst_x, self.obst_y = [], []\n",
    "        for _ in range(self.num_rectangles):\n",
    "            x1, y1 = np.random.uniform(self.xlim[0], self.xlim[1]), np.random.uniform(self.ylim[0], self.ylim[1])\n",
    "            x2, y2 = x1 + np.random.uniform(5, 15), y1 + np.random.uniform(5, 15)\n",
    "            self.obst_x.extend([x1, x1, x2, x2])\n",
    "            self.obst_y.extend([y1, y2, y2, y1])\n",
    "\n",
    "        # Generate random circles\n",
    "        self.radii = [np.random.uniform(2, 7) for _ in range(self.num_circles)]\n",
    "        self.circle_x = [np.random.uniform(self.xlim[0], self.xlim[1]) for _ in range(self.num_circles)]\n",
    "        self.circle_y = [np.random.uniform(self.ylim[0], self.ylim[1]) for _ in range(self.num_circles)]\n",
    "\n",
    "    def random_start(self):\n",
    "        while True:\n",
    "            x_start, y_start = np.random.uniform(self.xlim[0], self.xlim[1]), np.random.uniform(self.ylim[0], self.ylim[1])\n",
    "            if not self.is_inside_obstacle(x_start, y_start, 0):  # Pass 0 as robot radius for initial placement\n",
    "                return x_start, y_start\n",
    "\n",
    "    def random_goal(self):\n",
    "        while True:\n",
    "            x_goal, y_goal = np.random.uniform(self.xlim[0], self.xlim[1]), np.random.uniform(self.ylim[0], self.ylim[1])\n",
    "            if not self.is_inside_obstacle(x_goal, y_goal, 0):  # Pass 0 as robot radius for initial placement\n",
    "                dist = np.sqrt((x_goal - self.x_start)**2 + (y_goal - self.y_start)**2)\n",
    "                if dist >= self.min_distance:\n",
    "                    return x_goal, y_goal\n",
    "\n",
    "    def is_inside_obstacle(self, x, y, rbot):\n",
    "        # Check rectangular obstacles\n",
    "        for i in range(self.num_rectangles):\n",
    "            rect_x = self.obst_x[4 * i:4 * (i + 1)]\n",
    "            rect_y = self.obst_y[4 * i:4 * (i + 1)]\n",
    "            if (min(rect_x) - rbot) <= x <= (max(rect_x) + rbot) and \\\n",
    "               (min(rect_y) - rbot) <= y <= (max(rect_y) + rbot):\n",
    "                return True\n",
    "\n",
    "        # Check circular obstacles\n",
    "        for i in range(self.num_circles):\n",
    "            if (x - self.circle_x[i])**2 + (y - self.circle_y[i])**2 <= (self.radii[i] + rbot)**2:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def plot_obstacles(self, rbot):\n",
    "        plt.plot([0 - 2*rbot, 0 - 2*rbot, 100 + 2*rbot, 100 + 2*rbot, 0 - 2*rbot],\n",
    "                 [0 - 2*rbot, 100 + 2*rbot, 100 + 2*rbot, 0 - 2*rbot, 0 - 2*rbot], 'k', lw=0.5)\n",
    "        plt.plot(self.x_goal, self.y_goal, 'g*', markersize=20)\n",
    "        plt.plot(self.x_start, self.y_start, 'b*', markersize=20)\n",
    "\n",
    "        # Plot rectangles\n",
    "        for i in range(self.num_rectangles):\n",
    "            x = self.obst_x[4 * i]\n",
    "            y = self.obst_y[4 * i]\n",
    "            width = self.obst_x[4 * i + 2] - self.obst_x[4 * i]\n",
    "            height = self.obst_y[4 * i + 2] - self.obst_y[4 * i]\n",
    "            rect = plt.Rectangle((x, y), width, height, linewidth=1, color='r')\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "        # Plot circles\n",
    "        for i in range(self.num_circles):\n",
    "            circle = plt.Circle((self.circle_x[i], self.circle_y[i]), self.radii[i], color='r')\n",
    "            plt.gca().add_patch(circle)\n",
    "\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # plotting Functions    \n",
    "    def save_frame(self, directory, frame_count):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        frame_path = os.path.join(directory, f\"frame_{frame_count:04d}.png\")\n",
    "        plt.savefig(frame_path)\n",
    "        print(f\"Frame saved: {frame_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def draw(self, robot, tree=None, path=None, save_frames=False, frame_dir=\"rlframes\", frame_count=0):\n",
    "        self.plot_obstacles(robot.rbot)\n",
    "        \n",
    "        # Plot robot position\n",
    "        plt.plot(robot.pos[0], robot.pos[1], 'ko', markersize=10)\n",
    "        \n",
    "        # Plotting the RRT tree (lines only)\n",
    "        if tree:\n",
    "            for parent, child in tree.graph.edges():\n",
    "                plt.plot([parent.coordinates[0], child.coordinates[0]], \n",
    "                        [parent.coordinates[1], child.coordinates[1]], 'b-', lw=1)\n",
    "\n",
    "        # Plotting the path to the goal (nodes and lines)\n",
    "        if path:\n",
    "            for i in range(len(path) - 1):\n",
    "                plt.plot([path[i][0], path[i+1][0]], \n",
    "                        [path[i][1], path[i+1][1]], 'g-', lw=2)\n",
    "            for node in path:\n",
    "                plt.plot(node[0], node[1], 'go', markersize=5)\n",
    "\n",
    "        if save_frames:\n",
    "            self.save_frame(frame_dir, frame_count)\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self, env, rbot=3, max_dist=5):\n",
    "        self.env = env\n",
    "        self.rbot = rbot\n",
    "        self.max_dist = max_dist\n",
    "        self.pos = [env.x_start, env.y_start]\n",
    "        \n",
    "    def move(self, action):\n",
    "        new_pos = self.pos.copy()\n",
    "        if action == 0:\n",
    "            new_pos[0] += self.max_dist\n",
    "        elif action == 1:\n",
    "            new_pos[0] -= self.max_dist\n",
    "        elif action == 2:\n",
    "            new_pos[1] += self.max_dist\n",
    "        elif action == 3:\n",
    "            new_pos[1] -= self.max_dist\n",
    "            \n",
    "        if not self.env.is_inside_obstacle(new_pos[0], new_pos[1], self.rbot):\n",
    "            self.pos = new_pos\n",
    "            \n",
    "        return self.pos\n",
    "\n",
    "    def sample_grid(self, grid_size=5):\n",
    "        x_range = np.arange(self.env.xlim[0], self.env.xlim[1], grid_size)\n",
    "        y_range = np.arange(self.env.ylim[0], self.env.ylim[1], grid_size)\n",
    "        features = []\n",
    "        edge_index = []\n",
    "\n",
    "        # Create feature matrix and adjacency matrix\n",
    "        node_index = 0\n",
    "        for i in x_range:\n",
    "            for j in y_range:\n",
    "                features.append([i, j, self.pos[0], self.pos[1]])\n",
    "                # Connect to neighboring nodes (4-neighborhood)\n",
    "                if i + grid_size < self.env.xlim[1]:\n",
    "                    edge_index.append([node_index, node_index + len(y_range)])\n",
    "                    edge_index.append([node_index + len(y_range), node_index])\n",
    "                if j + grid_size < self.env.ylim[1]:\n",
    "                    edge_index.append([node_index, node_index + 1])\n",
    "                    edge_index.append([node_index + 1, node_index])\n",
    "                node_index += 1\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        return features, edge_index\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        new_pos = self.move(action)\n",
    "        x, y = new_pos\n",
    "\n",
    "        if self.env.is_inside_obstacle(x, y, self.rbot):\n",
    "            return -100  # Hitting an obstacle\n",
    "\n",
    "        if [x, y] == [self.env.x_goal, self.env.y_goal]:\n",
    "            return 1000  # Reaching the goal\n",
    "\n",
    "        return -1  # Step penalty\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.get_reward(action)\n",
    "        new_state = self.sample_grid()\n",
    "        goal_check = self.pos == [self.env.x_goal, self.env.y_goal] or reward == -100\n",
    "        return new_state, reward, goal_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the Environment and Robot Class\n",
    "env = Environment()\n",
    "robot = Robot(env)\n",
    "env.draw(robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    \"\"\"Graph Auto-Encoder for environment representation\"\"\"\n",
    "    def __init__(self):\n",
    "        super(GAE, self).__init__()\n",
    "        # Input: [num_nodes, 4] -> Hidden: [num_nodes, 8] -> Output: [num_nodes, 2]\n",
    "        self.encoder = torch.nn.ModuleList([\n",
    "            GATConv(in_channels=4, out_channels=8, heads=1),\n",
    "            GATConv(in_channels=8, out_channels=2, heads=1)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, node_features, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_features: [num_nodes, 4] - (x, y, robot_x, robot_y)\n",
    "            edge_index: [2, num_edges] - Graph connectivity\n",
    "        Returns:\n",
    "            embeddings: [num_nodes, 2] - Learned node representations\n",
    "        \"\"\"\n",
    "        x = node_features\n",
    "        for layer in self.encoder:\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gae(epochs, robot_instance, model, optimizer, criterion, grid_size=5):\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get current state representation\n",
    "        node_features, edge_index = robot_instance.sample_grid(grid_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = model(node_features, edge_index)\n",
    "        \n",
    "        # Create target adjacency matrix\n",
    "        edge_index_dense = to_dense_adj(edge_index)[0]\n",
    "        \n",
    "        # Reconstruct adjacency from embeddings\n",
    "        pred_adj = torch.sigmoid(torch.mm(embeddings, embeddings.t()))\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.binary_cross_entropy(pred_adj, edge_index_dense)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), 'best_gae_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(gae_model, features, edge_index):\n",
    "    # Set the model to evaluation mode\n",
    "    gae_model.eval()\n",
    "\n",
    "    # Generate embeddings using the GAE model\n",
    "    with torch.no_grad():\n",
    "        embeddings = gae_model(features, edge_index)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010, Loss: 53.9870\n",
      "Epoch 020, Loss: 52.8133\n",
      "Epoch 030, Loss: 52.1670\n",
      "Epoch 040, Loss: 51.5597\n",
      "Epoch 050, Loss: 41.8089\n",
      "Epoch 060, Loss: 41.3250\n",
      "Epoch 070, Loss: 32.7470\n",
      "Epoch 080, Loss: 32.6841\n",
      "Epoch 090, Loss: 43.3361\n",
      "Epoch 100, Loss: 25.7720\n",
      "Epoch 110, Loss: 35.2944\n",
      "Epoch 120, Loss: 23.0003\n",
      "Epoch 130, Loss: 26.6015\n",
      "Epoch 140, Loss: 22.5320\n",
      "Epoch 150, Loss: 13.2868\n",
      "Epoch 160, Loss: 7.7310\n",
      "Epoch 170, Loss: 7.8271\n",
      "Epoch 180, Loss: 3.4664\n",
      "Epoch 190, Loss: 2.2186\n",
      "Epoch 200, Loss: 4.8459\n",
      "Epoch 210, Loss: 4.0337\n",
      "Epoch 220, Loss: 4.8535\n",
      "Epoch 230, Loss: 4.9068\n",
      "Epoch 240, Loss: 4.6636\n",
      "Epoch 250, Loss: 3.6207\n",
      "Epoch 260, Loss: 4.0989\n",
      "Epoch 270, Loss: 4.4839\n",
      "Epoch 280, Loss: 2.9578\n",
      "Epoch 290, Loss: 2.9362\n",
      "Epoch 300, Loss: 3.6307\n",
      "Epoch 310, Loss: 3.8914\n",
      "Epoch 320, Loss: 3.0826\n",
      "Epoch 330, Loss: 1.5875\n",
      "Epoch 340, Loss: 3.2014\n",
      "Epoch 350, Loss: 2.3051\n",
      "Epoch 360, Loss: 2.0582\n",
      "Epoch 370, Loss: 2.2761\n",
      "Epoch 380, Loss: 2.7048\n",
      "Epoch 390, Loss: 3.2360\n",
      "Epoch 400, Loss: 4.1065\n",
      "Epoch 410, Loss: 3.2421\n",
      "Epoch 420, Loss: 1.7639\n",
      "Epoch 430, Loss: 1.5908\n",
      "Epoch 440, Loss: 3.2403\n",
      "Epoch 450, Loss: 1.7300\n",
      "Epoch 460, Loss: 2.8134\n",
      "Epoch 470, Loss: 2.2732\n",
      "Epoch 480, Loss: 2.4778\n",
      "Epoch 490, Loss: 2.1353\n",
      "Epoch 500, Loss: 2.3060\n",
      "Epoch 510, Loss: 2.2683\n",
      "Epoch 520, Loss: 2.1826\n",
      "Epoch 530, Loss: 2.0555\n",
      "Epoch 540, Loss: 3.2452\n",
      "Epoch 550, Loss: 2.6248\n",
      "Epoch 560, Loss: 1.4244\n",
      "Epoch 570, Loss: 1.8261\n",
      "Epoch 580, Loss: 2.2942\n",
      "Epoch 590, Loss: 2.9072\n",
      "Epoch 600, Loss: 2.7774\n",
      "Epoch 610, Loss: 1.6981\n",
      "Epoch 620, Loss: 1.7872\n",
      "Epoch 630, Loss: 1.9662\n",
      "Epoch 640, Loss: 1.5146\n",
      "Epoch 650, Loss: 2.8018\n",
      "Epoch 660, Loss: 1.8215\n",
      "Epoch 670, Loss: 3.0272\n",
      "Epoch 680, Loss: 2.0032\n",
      "Epoch 690, Loss: 2.3525\n",
      "Epoch 700, Loss: 1.8751\n",
      "Epoch 710, Loss: 1.8508\n",
      "Epoch 720, Loss: 1.7129\n",
      "Epoch 730, Loss: 1.5562\n",
      "Epoch 740, Loss: 1.4460\n",
      "Epoch 750, Loss: 1.3121\n",
      "Epoch 760, Loss: 2.1438\n",
      "Epoch 770, Loss: 3.0602\n",
      "Epoch 780, Loss: 1.5408\n",
      "Epoch 790, Loss: 1.6506\n",
      "Epoch 800, Loss: 1.4092\n",
      "Epoch 810, Loss: 1.8353\n",
      "Epoch 820, Loss: 1.6376\n",
      "Epoch 830, Loss: 2.2181\n",
      "Epoch 840, Loss: 1.6090\n",
      "Epoch 850, Loss: 1.4608\n",
      "Epoch 860, Loss: 1.5106\n",
      "Epoch 870, Loss: 2.2773\n",
      "Epoch 880, Loss: 1.5442\n",
      "Epoch 890, Loss: 1.7798\n",
      "Epoch 900, Loss: 2.2371\n",
      "Epoch 910, Loss: 1.5464\n",
      "Epoch 920, Loss: 1.7537\n",
      "Epoch 930, Loss: 1.4512\n",
      "Epoch 940, Loss: 2.3101\n",
      "Epoch 950, Loss: 1.9970\n",
      "Epoch 960, Loss: 2.0160\n",
      "Epoch 970, Loss: 1.3110\n",
      "Epoch 980, Loss: 1.9484\n",
      "Epoch 990, Loss: 1.3901\n",
      "Epoch 1000, Loss: 1.6954\n",
      "Epoch 1010, Loss: 2.1731\n",
      "Epoch 1020, Loss: 1.7410\n",
      "Epoch 1030, Loss: 1.2566\n",
      "Epoch 1040, Loss: 1.9885\n",
      "Epoch 1050, Loss: 1.3721\n",
      "Epoch 1060, Loss: 1.4347\n",
      "Epoch 1070, Loss: 1.8630\n",
      "Epoch 1080, Loss: 1.5125\n",
      "Epoch 1090, Loss: 2.2416\n",
      "Epoch 1100, Loss: 2.0348\n",
      "Epoch 1110, Loss: 1.2590\n",
      "Epoch 1120, Loss: 1.8124\n",
      "Epoch 1130, Loss: 1.6894\n",
      "Epoch 1140, Loss: 1.9227\n",
      "Epoch 1150, Loss: 1.8035\n",
      "Epoch 1160, Loss: 2.7098\n",
      "Epoch 1170, Loss: 1.6051\n",
      "Epoch 1180, Loss: 1.3184\n",
      "Epoch 1190, Loss: 1.8531\n",
      "Epoch 1200, Loss: 2.1261\n",
      "Epoch 1210, Loss: 2.0601\n",
      "Epoch 1220, Loss: 1.8559\n",
      "Epoch 1230, Loss: 1.4801\n",
      "Epoch 1240, Loss: 1.5750\n",
      "Epoch 1250, Loss: 1.3645\n",
      "Epoch 1260, Loss: 1.9896\n",
      "Epoch 1270, Loss: 1.1887\n",
      "Epoch 1280, Loss: 1.2564\n",
      "Epoch 1290, Loss: 1.5597\n",
      "Epoch 1300, Loss: 2.1746\n",
      "Epoch 1310, Loss: 2.2362\n",
      "Epoch 1320, Loss: 1.8679\n",
      "Epoch 1330, Loss: 1.4442\n",
      "Epoch 1340, Loss: 1.6871\n",
      "Epoch 1350, Loss: 1.7081\n",
      "Epoch 1360, Loss: 1.0797\n",
      "Epoch 1370, Loss: 1.3271\n",
      "Epoch 1380, Loss: 1.7896\n",
      "Epoch 1390, Loss: 1.3048\n",
      "Epoch 1400, Loss: 1.2737\n",
      "Epoch 1410, Loss: 2.0433\n",
      "Epoch 1420, Loss: 1.1986\n",
      "Epoch 1430, Loss: 1.0454\n",
      "Epoch 1440, Loss: 1.5697\n",
      "Epoch 1450, Loss: 1.1435\n",
      "Epoch 1460, Loss: 1.3097\n",
      "Epoch 1470, Loss: 2.4627\n",
      "Epoch 1480, Loss: 1.6078\n",
      "Epoch 1490, Loss: 1.3429\n",
      "Epoch 1500, Loss: 1.2661\n",
      "Epoch 1510, Loss: 2.2063\n",
      "Epoch 1520, Loss: 1.3071\n",
      "Epoch 1530, Loss: 1.9360\n",
      "Epoch 1540, Loss: 2.2918\n",
      "Epoch 1550, Loss: 1.4498\n",
      "Epoch 1560, Loss: 1.6874\n",
      "Epoch 1570, Loss: 1.4758\n",
      "Epoch 1580, Loss: 1.2702\n",
      "Epoch 1590, Loss: 1.3274\n",
      "Epoch 1600, Loss: 1.8471\n",
      "Epoch 1610, Loss: 1.1339\n",
      "Epoch 1620, Loss: 2.0146\n",
      "Epoch 1630, Loss: 1.3416\n",
      "Epoch 1640, Loss: 1.3193\n",
      "Epoch 1650, Loss: 1.5435\n",
      "Epoch 1660, Loss: 1.1580\n",
      "Epoch 1670, Loss: 1.1488\n",
      "Epoch 1680, Loss: 1.3465\n",
      "Epoch 1690, Loss: 2.3907\n",
      "Epoch 1700, Loss: 1.5062\n",
      "Epoch 1710, Loss: 1.4477\n",
      "Epoch 1720, Loss: 1.2705\n",
      "Epoch 1730, Loss: 1.5927\n",
      "Epoch 1740, Loss: 1.3212\n",
      "Epoch 1750, Loss: 1.8392\n",
      "Epoch 1760, Loss: 1.1546\n",
      "Epoch 1770, Loss: 1.6096\n",
      "Epoch 1780, Loss: 1.9566\n",
      "Epoch 1790, Loss: 1.5931\n",
      "Epoch 1800, Loss: 1.4227\n",
      "Epoch 1810, Loss: 1.4277\n",
      "Epoch 1820, Loss: 1.2783\n",
      "Epoch 1830, Loss: 1.4699\n",
      "Epoch 1840, Loss: 0.9883\n",
      "Epoch 1850, Loss: 1.2758\n",
      "Epoch 1860, Loss: 1.3484\n",
      "Epoch 1870, Loss: 0.8720\n",
      "Epoch 1880, Loss: 1.7031\n",
      "Epoch 1890, Loss: 1.5503\n",
      "Epoch 1900, Loss: 1.5766\n",
      "Epoch 1910, Loss: 1.4948\n",
      "Epoch 1920, Loss: 1.4627\n",
      "Epoch 1930, Loss: 2.0995\n",
      "Epoch 1940, Loss: 1.1888\n",
      "Epoch 1950, Loss: 1.3352\n",
      "Epoch 1960, Loss: 0.9793\n",
      "Epoch 1970, Loss: 1.1162\n",
      "Epoch 1980, Loss: 1.5295\n",
      "Epoch 1990, Loss: 1.3727\n",
      "Epoch 2000, Loss: 1.1197\n",
      "Epoch 2010, Loss: 1.4665\n",
      "Epoch 2020, Loss: 1.5089\n",
      "Epoch 2030, Loss: 1.2309\n",
      "Epoch 2040, Loss: 1.7080\n",
      "Epoch 2050, Loss: 1.6090\n",
      "Epoch 2060, Loss: 1.2808\n",
      "Epoch 2070, Loss: 1.1660\n",
      "Epoch 2080, Loss: 1.0548\n",
      "Epoch 2090, Loss: 1.4483\n",
      "Epoch 2100, Loss: 0.8942\n",
      "Epoch 2110, Loss: 1.1079\n",
      "Epoch 2120, Loss: 1.3675\n",
      "Epoch 2130, Loss: 1.1357\n",
      "Epoch 2140, Loss: 1.1153\n",
      "Epoch 2150, Loss: 0.8987\n",
      "Epoch 2160, Loss: 0.9167\n",
      "Epoch 2170, Loss: 1.6505\n",
      "Epoch 2180, Loss: 1.0169\n",
      "Epoch 2190, Loss: 1.4220\n",
      "Epoch 2200, Loss: 1.0596\n",
      "Epoch 2210, Loss: 0.9987\n",
      "Epoch 2220, Loss: 1.1132\n",
      "Epoch 2230, Loss: 1.2570\n",
      "Epoch 2240, Loss: 1.3450\n",
      "Epoch 2250, Loss: 0.9289\n",
      "Epoch 2260, Loss: 1.0274\n",
      "Epoch 2270, Loss: 1.0160\n",
      "Epoch 2280, Loss: 1.4037\n",
      "Epoch 2290, Loss: 1.1822\n",
      "Epoch 2300, Loss: 1.1255\n",
      "Epoch 2310, Loss: 0.9517\n",
      "Epoch 2320, Loss: 0.9706\n",
      "Epoch 2330, Loss: 0.8955\n",
      "Epoch 2340, Loss: 1.5646\n",
      "Epoch 2350, Loss: 1.0571\n",
      "Epoch 2360, Loss: 1.3045\n",
      "Epoch 2370, Loss: 0.9900\n",
      "Epoch 2380, Loss: 1.4348\n",
      "Epoch 2390, Loss: 0.8228\n",
      "Epoch 2400, Loss: 0.9621\n",
      "Epoch 2410, Loss: 0.9280\n",
      "Epoch 2420, Loss: 1.1736\n",
      "Epoch 2430, Loss: 0.9084\n",
      "Epoch 2440, Loss: 1.2024\n",
      "Epoch 2450, Loss: 0.8384\n",
      "Epoch 2460, Loss: 0.9000\n",
      "Epoch 2470, Loss: 1.2799\n",
      "Epoch 2480, Loss: 1.0482\n",
      "Epoch 2490, Loss: 1.0165\n",
      "Epoch 2500, Loss: 1.1063\n",
      "Epoch 2510, Loss: 0.9428\n",
      "Epoch 2520, Loss: 1.0121\n",
      "Epoch 2530, Loss: 0.8290\n",
      "Epoch 2540, Loss: 0.9048\n",
      "Epoch 2550, Loss: 0.9084\n",
      "Epoch 2560, Loss: 0.8459\n",
      "Epoch 2570, Loss: 0.8965\n",
      "Epoch 2580, Loss: 1.0726\n",
      "Epoch 2590, Loss: 0.9402\n",
      "Epoch 2600, Loss: 0.8062\n",
      "Epoch 2610, Loss: 0.9312\n",
      "Epoch 2620, Loss: 1.1044\n",
      "Epoch 2630, Loss: 0.7930\n",
      "Epoch 2640, Loss: 0.7525\n",
      "Epoch 2650, Loss: 0.8692\n",
      "Epoch 2660, Loss: 0.7571\n",
      "Epoch 2670, Loss: 0.8148\n",
      "Epoch 2680, Loss: 0.9521\n",
      "Epoch 2690, Loss: 0.9864\n",
      "Epoch 2700, Loss: 0.7909\n",
      "Epoch 2710, Loss: 0.8657\n",
      "Epoch 2720, Loss: 0.9141\n",
      "Epoch 2730, Loss: 0.7643\n",
      "Epoch 2740, Loss: 0.7687\n",
      "Epoch 2750, Loss: 0.7355\n",
      "Epoch 2760, Loss: 0.9583\n",
      "Epoch 2770, Loss: 0.7838\n",
      "Epoch 2780, Loss: 0.8269\n",
      "Epoch 2790, Loss: 0.7663\n",
      "Epoch 2800, Loss: 0.9038\n",
      "Epoch 2810, Loss: 0.8939\n",
      "Epoch 2820, Loss: 0.8945\n",
      "Epoch 2830, Loss: 0.7548\n",
      "Epoch 2840, Loss: 0.8348\n",
      "Epoch 2850, Loss: 0.8168\n",
      "Epoch 2860, Loss: 0.8640\n",
      "Epoch 2870, Loss: 0.9109\n",
      "Epoch 2880, Loss: 0.9571\n",
      "Epoch 2890, Loss: 1.0468\n",
      "Epoch 2900, Loss: 0.8165\n",
      "Epoch 2910, Loss: 0.9509\n",
      "Epoch 2920, Loss: 1.2216\n",
      "Epoch 2930, Loss: 0.8669\n",
      "Epoch 2940, Loss: 0.8598\n",
      "Epoch 2950, Loss: 0.7449\n",
      "Epoch 2960, Loss: 1.2383\n",
      "Epoch 2970, Loss: 0.7745\n",
      "Epoch 2980, Loss: 0.7474\n",
      "Epoch 2990, Loss: 0.8409\n",
      "Epoch 3000, Loss: 0.9189\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "gaemodel = GAE()\n",
    "optimizer = torch.optim.Adam(gaemodel.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "train_gae(epochs=3000, robot_instance=robot, model=gaemodel, optimizer=optimizer, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (torch.cat(states).to(self.device), \n",
    "                torch.tensor(actions).to(self.device), \n",
    "                torch.tensor(rewards).to(self.device),\n",
    "                torch.cat(next_states).to(self.device), \n",
    "                torch.tensor(dones).to(self.device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, output_dim=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(state).max(1)[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, output_dim=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(state).max(1)[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, output_dim=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(state).max(1)[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(gae_model, num_episodes=1000, batch_size=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move models to device\n",
    "    gae_model = gae_model.to(device)\n",
    "    dqn = DQN().to(device)\n",
    "    target_dqn = DQN().to(device)\n",
    "    target_dqn.load_state_dict(dqn.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(dqn.parameters())\n",
    "    memory = ReplayBuffer(10000, device)\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    \n",
    "    episode = 0\n",
    "    while episode < num_episodes:\n",
    "        env = Environment()\n",
    "        robot = Robot(env)\n",
    "        state = robot.sample_grid()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_embedding = gae_model(state[0].to(device), state[1].to(device))\n",
    "            state_embedding = state_embedding.mean(dim=0).unsqueeze(0)\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, 3)\n",
    "            else:\n",
    "                action = dqn.predict(state_embedding)\n",
    "            \n",
    "            robot.move(action)\n",
    "            reward = robot.get_reward(action)\n",
    "            next_state = robot.sample_grid()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_state_embedding = gae_model(next_state[0].to(device), next_state[1].to(device))\n",
    "                next_state_embedding = next_state_embedding.mean(dim=0).unsqueeze(0)\n",
    "            \n",
    "            done = reward in [-100, 1000]\n",
    "            \n",
    "            memory.push(state_embedding, action, reward, next_state_embedding, done)\n",
    "            \n",
    "            if len(memory) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "                \n",
    "                current_q = dqn(states).gather(1, actions.unsqueeze(1))\n",
    "                next_q = target_dqn(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + gamma * next_q * (1 - dones.float())\n",
    "                \n",
    "                loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                print(f\"Episode {episode}, Step Loss: {loss.item()}\")\n",
    "            \n",
    "            state_embedding = next_state_embedding\n",
    "            total_reward += reward\n",
    "            \n",
    "        if episode % 10 == 0:\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "        \n",
    "        episode += 1\n",
    "    \n",
    "    return dqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Step Loss: 0.7283788323402405\n",
      "Episode 0, Step Loss: 0.7067155241966248\n",
      "Episode 0, Step Loss: 0.6880978941917419\n",
      "Episode 0, Step Loss: 0.6418212652206421\n",
      "Episode 0, Step Loss: 0.6423096656799316\n",
      "Episode 0, Step Loss: 0.5941129326820374\n",
      "Episode 0, Step Loss: 0.5934010148048401\n",
      "Episode 0, Step Loss: 0.571648359298706\n",
      "Episode 0, Step Loss: 0.5177505016326904\n",
      "Episode 0, Step Loss: 0.5240190029144287\n",
      "Episode 0, Step Loss: 0.48162782192230225\n",
      "Episode 0, Step Loss: 0.4485623836517334\n",
      "Episode 0, Step Loss: 0.44983574748039246\n",
      "Episode 0, Step Loss: 0.4323616325855255\n",
      "Episode 0, Step Loss: 0.4106687307357788\n",
      "Episode 0, Step Loss: 0.39103323221206665\n",
      "Episode 0, Step Loss: 0.35713765025138855\n",
      "Episode 0, Step Loss: 0.34101101756095886\n",
      "Episode 0, Step Loss: 0.3076721429824829\n",
      "Episode 0, Step Loss: 0.30084705352783203\n",
      "Episode 0, Step Loss: 0.2712188959121704\n",
      "Episode 0, Step Loss: 0.273029625415802\n",
      "Episode 0, Step Loss: 0.2307092696428299\n",
      "Episode 0, Step Loss: 0.20264007151126862\n",
      "Episode 0, Step Loss: 0.1870594620704651\n",
      "Episode 0, Step Loss: 0.16453373432159424\n",
      "Episode 0, Step Loss: 0.1573849767446518\n",
      "Episode 0, Step Loss: 0.14525136351585388\n",
      "Episode 0, Step Loss: 0.12505927681922913\n",
      "Episode 0, Step Loss: 0.10536058992147446\n",
      "Episode 0, Step Loss: 0.08986558020114899\n",
      "Episode 0, Step Loss: 0.08852998912334442\n",
      "Episode 0, Step Loss: 0.07118996977806091\n",
      "Episode 0, Step Loss: 0.058984119445085526\n",
      "Episode 0, Step Loss: 0.04734095185995102\n",
      "Episode 0, Step Loss: 0.04154205322265625\n",
      "Episode 0, Step Loss: 0.02737721987068653\n",
      "Episode 0, Step Loss: 0.02293115295469761\n",
      "Episode 0, Step Loss: 0.021688314154744148\n",
      "Episode 0, Step Loss: 0.012011456303298473\n",
      "Episode 0, Step Loss: 0.009906098246574402\n",
      "Episode 0, Step Loss: 0.008081118576228619\n",
      "Episode 0, Step Loss: 0.0048377178609371185\n",
      "Episode 0, Step Loss: 0.005844619125127792\n",
      "Episode 0, Step Loss: 0.005283004138618708\n",
      "Episode 0, Step Loss: 0.005849403329193592\n",
      "Episode 0, Step Loss: 0.00762585923075676\n",
      "Episode 0, Step Loss: 0.006793392822146416\n",
      "Episode 0, Step Loss: 0.00954739935696125\n",
      "Episode 0, Step Loss: 0.009027007967233658\n",
      "Episode 0, Step Loss: 0.008123055100440979\n",
      "Episode 0, Step Loss: 0.010055403225123882\n",
      "Episode 0, Step Loss: 0.010425429791212082\n",
      "Episode 0, Step Loss: 0.009595810435712337\n",
      "Episode 0, Step Loss: 0.007005077786743641\n",
      "Episode 0, Step Loss: 0.007840102538466454\n",
      "Episode 0, Step Loss: 0.007264649961143732\n",
      "Episode 0, Step Loss: 0.006011728197336197\n",
      "Episode 0, Step Loss: 0.006138015538454056\n",
      "Episode 0, Step Loss: 0.004892203491181135\n",
      "Episode 0, Step Loss: 0.004670965950936079\n",
      "Episode 0, Step Loss: 0.0025627613067626953\n",
      "Episode 0, Step Loss: 0.003342435695230961\n",
      "Episode 0, Step Loss: 0.0019494248554110527\n",
      "Episode 0, Step Loss: 0.0009608052205294371\n",
      "Episode 0, Step Loss: 0.0010489828418940306\n",
      "Episode 0, Step Loss: 0.0009232683223672211\n",
      "Episode 0, Step Loss: 0.0018252972513437271\n",
      "Episode 0, Step Loss: 0.0012932356912642717\n",
      "Episode 0, Step Loss: 0.0010841996408998966\n",
      "Episode 0, Step Loss: 0.0019725218880921602\n",
      "Episode 0, Step Loss: 0.001155183301307261\n",
      "Episode 0, Step Loss: 0.001042374991811812\n",
      "Episode 0, Step Loss: 0.0013600748497992754\n",
      "Episode 0, Step Loss: 0.0020192074589431286\n",
      "Episode 0, Step Loss: 0.001599172712303698\n",
      "Episode 0, Step Loss: 0.0019479996990412474\n",
      "Episode 0, Step Loss: 0.0018783464329317212\n",
      "Episode 0, Step Loss: 0.0010044958908110857\n",
      "Episode 0, Step Loss: 0.0014502445701509714\n",
      "Episode 0, Step Loss: 0.0016140767838805914\n",
      "Episode 0, Step Loss: 0.0013274307129904628\n",
      "Episode 0, Step Loss: 0.0010969277936965227\n",
      "Episode 0, Step Loss: 0.0009771735640242696\n",
      "Episode 0, Step Loss: 0.0012734322808682919\n",
      "Episode 0, Step Loss: 0.0007852476555854082\n",
      "Episode 0, Step Loss: 0.000497826433274895\n",
      "Episode 0, Step Loss: 0.000601323670707643\n",
      "Episode 0, Step Loss: 0.00034766647149808705\n",
      "Episode 0, Step Loss: 0.0008555730455555022\n",
      "Episode 0, Step Loss: 0.0008239621529355645\n",
      "Episode 0, Step Loss: 0.0011011853348463774\n",
      "Episode 0, Step Loss: 0.00057676323922351\n",
      "Episode 0, Step Loss: 0.0003943145275115967\n",
      "Episode 0, Step Loss: 0.0007857135497033596\n",
      "Episode 0, Step Loss: 0.0009676150511950254\n",
      "Episode 0, Step Loss: 0.0008000587695278227\n",
      "Episode 0, Step Loss: 0.0008678571321070194\n",
      "Episode 0, Step Loss: 0.0007586838328279555\n",
      "Episode 0, Step Loss: 0.0010652766795828938\n",
      "Episode 0, Step Loss: 0.0008571971557103097\n",
      "Episode 0, Step Loss: 0.0008745817467570305\n",
      "Episode 0, Step Loss: 0.0006672914023511112\n",
      "Episode 0, Step Loss: 0.0006006542243994772\n",
      "Episode 0, Step Loss: 0.00045865020365454257\n",
      "Episode 0, Step Loss: 0.0008352688164450228\n",
      "Episode 0, Step Loss: 0.0005669177044183016\n",
      "Episode 0, Step Loss: 0.0010330048389732838\n",
      "Episode 0, Step Loss: 0.0005843132385052741\n",
      "Episode 0, Step Loss: 0.0006656626937910914\n",
      "Episode 0, Step Loss: 0.0004901638603769243\n",
      "Episode 0, Step Loss: 0.0009029767825268209\n",
      "Episode 0, Step Loss: 0.0010303641902282834\n",
      "Episode 0, Step Loss: 0.0006153989234007895\n",
      "Episode 0, Step Loss: 0.00038799646426923573\n",
      "Episode 0, Step Loss: 0.000553219928406179\n",
      "Episode 0, Step Loss: 0.00048174854600802064\n",
      "Episode 0, Step Loss: 0.0005203232285566628\n",
      "Episode 0, Step Loss: 0.00033907889155671\n",
      "Episode 0, Step Loss: 0.0004278909764252603\n",
      "Episode 0, Step Loss: 0.0004890559357590973\n",
      "Episode 0, Step Loss: 0.0005675096763297915\n",
      "Episode 0, Step Loss: 0.00031298381509259343\n",
      "Episode 0, Step Loss: 0.00037031248211860657\n",
      "Episode 0, Step Loss: 0.0006504811462946236\n",
      "Episode 0, Step Loss: 0.0003589630941860378\n",
      "Episode 0, Step Loss: 0.00023668084759265184\n",
      "Episode 0, Step Loss: 0.0006455792463384569\n",
      "Episode 0, Step Loss: 0.0006608749972656369\n",
      "Episode 0, Step Loss: 0.00048380691441707313\n",
      "Episode 0, Step Loss: 0.00034147832775488496\n",
      "Episode 0, Step Loss: 0.0005189580260775983\n",
      "Episode 0, Step Loss: 0.0003141522756777704\n",
      "Episode 0, Step Loss: 0.0001786402426660061\n",
      "Episode 0, Step Loss: 0.0005378307541832328\n",
      "Episode 0, Step Loss: 0.000539083150215447\n",
      "Episode 0, Step Loss: 0.00045472741476260126\n",
      "Episode 0, Step Loss: 0.0005416306084953249\n",
      "Episode 0, Step Loss: 0.0004786762874573469\n",
      "Episode 0, Step Loss: 0.00028734884108416736\n",
      "Episode 0, Step Loss: 0.0004351057286839932\n",
      "Episode 0, Step Loss: 0.00033043077564798295\n",
      "Episode 0, Step Loss: 0.000308218935970217\n",
      "Episode 0, Step Loss: 0.0003204401582479477\n",
      "Episode 0, Step Loss: 0.0003374352818354964\n",
      "Episode 0, Step Loss: 0.00039524026215076447\n",
      "Episode 0, Step Loss: 0.000316958234179765\n",
      "Episode 0, Step Loss: 0.00033536076080054045\n",
      "Episode 0, Step Loss: 0.0004501408839132637\n",
      "Episode 0, Step Loss: 0.000547858770005405\n",
      "Episode 0, Step Loss: 0.0005460679531097412\n",
      "Episode 0, Step Loss: 0.00017136672977358103\n",
      "Episode 0, Step Loss: 0.0003423111920710653\n",
      "Episode 0, Step Loss: 0.0003068717196583748\n",
      "Episode 0, Step Loss: 0.0004213974461890757\n",
      "Episode 0, Step Loss: 0.00041067705024033785\n",
      "Episode 0, Step Loss: 0.0001294680987484753\n",
      "Episode 0, Step Loss: 0.00034394225804135203\n",
      "Episode 0, Step Loss: 0.0002747420221567154\n",
      "Episode 0, Step Loss: 0.0001780696038622409\n",
      "Episode 0, Step Loss: 0.0002670905087143183\n",
      "Episode 0, Step Loss: 0.00022146922128740698\n",
      "Episode 0, Step Loss: 0.0007127788267098367\n",
      "Episode 0, Step Loss: 0.0005658791633322835\n",
      "Episode 0, Step Loss: 0.0003864628670271486\n",
      "Episode 0, Step Loss: 0.0005257034208625555\n",
      "Episode 0, Step Loss: 0.0002022197877522558\n",
      "Episode 0, Step Loss: 0.0003504496708046645\n",
      "Episode 0, Step Loss: 0.00031445123022422194\n",
      "Episode 0, Step Loss: 0.0003605581587180495\n",
      "Episode 0, Step Loss: 0.00030795924249105155\n",
      "Episode 0, Step Loss: 0.00044218669063411653\n",
      "Episode 0, Step Loss: 0.0001520747464383021\n",
      "Episode 0, Step Loss: 0.00040477971197105944\n",
      "Episode 0, Step Loss: 0.0004011385899502784\n",
      "Episode 0, Step Loss: 0.00018013716908171773\n",
      "Episode 0, Step Loss: 0.0004348171059973538\n",
      "Episode 0, Step Loss: 0.0001975830818992108\n",
      "Episode 0, Step Loss: 0.00014759421173948795\n",
      "Episode 0, Step Loss: 0.00047585240099579096\n",
      "Episode 0, Step Loss: 0.0002955155214294791\n",
      "Episode 0, Step Loss: 0.0003151570854242891\n",
      "Episode 0, Step Loss: 0.00028133843443356454\n",
      "Episode 0, Step Loss: 0.00027427199529483914\n",
      "Episode 0, Step Loss: 0.0003642118535935879\n",
      "Episode 0, Step Loss: 0.00020216696429997683\n",
      "Episode 0, Step Loss: 0.0003993040299974382\n",
      "Episode 0, Step Loss: 0.00032316267606802285\n",
      "Episode 0, Step Loss: 0.0002864788402803242\n",
      "Episode 0, Step Loss: 0.00017663536709733307\n",
      "Episode 0, Step Loss: 0.00020991466590203345\n",
      "Episode 0, Step Loss: 0.0002632386749610305\n",
      "Episode 0, Step Loss: 0.00023288640659302473\n",
      "Episode 0, Step Loss: 0.00020656295237131417\n",
      "Episode 0, Step Loss: 0.0003630775900091976\n",
      "Episode 0, Step Loss: 0.00010713674419093877\n",
      "Episode 0, Step Loss: 0.00037444254849106073\n",
      "Episode 0, Step Loss: 0.0002800889778882265\n",
      "Episode 0, Step Loss: 0.00036370084853842854\n",
      "Episode 0, Step Loss: 0.0002676095173228532\n",
      "Episode 0, Step Loss: 0.00019919122860301286\n",
      "Episode 0, Step Loss: 0.00017869181465357542\n",
      "Episode 0, Step Loss: 0.0002961798454634845\n",
      "Episode 0, Step Loss: 0.00019290977797936648\n",
      "Episode 0, Step Loss: 0.00029979427927173674\n",
      "Episode 0, Step Loss: 0.000243485439568758\n",
      "Episode 0, Step Loss: 0.00017126179591286927\n",
      "Episode 0, Step Loss: 0.0002386730193393305\n",
      "Episode 0, Step Loss: 0.00038758470327593386\n",
      "Episode 0, Step Loss: 0.00017671319073997438\n",
      "Episode 0, Step Loss: 0.00018922326853498816\n",
      "Episode 0, Step Loss: 0.0003931073297280818\n",
      "Episode 0, Step Loss: 0.0002804454124998301\n",
      "Episode 0, Step Loss: 7.347416249103844e-05\n",
      "Episode 0, Step Loss: 0.0001036512985592708\n",
      "Episode 0, Step Loss: 0.0002767746045719832\n",
      "Episode 0, Step Loss: 0.00020470572053454816\n",
      "Episode 0, Step Loss: 0.00011354318121448159\n",
      "Episode 0, Step Loss: 0.00019432992849033326\n",
      "Episode 0, Step Loss: 0.0002246701915282756\n",
      "Episode 0, Step Loss: 0.00038557947846129537\n",
      "Episode 0, Step Loss: 0.000272440753178671\n",
      "Episode 0, Step Loss: 0.0004772185056935996\n",
      "Episode 0, Step Loss: 0.0002480976691003889\n",
      "Episode 0, Step Loss: 0.00018385829753242433\n",
      "Episode 0, Step Loss: 0.0002977330586872995\n",
      "Episode 0, Step Loss: 0.00027586359647102654\n",
      "Episode 0, Step Loss: 0.0002843638649210334\n",
      "Episode 0, Step Loss: 0.00014414143515750766\n",
      "Episode 0, Step Loss: 0.00036413970519788563\n",
      "Episode 0, Step Loss: 0.0003224323154427111\n",
      "Episode 0, Step Loss: 0.00025349773932248354\n",
      "Episode 0, Step Loss: 0.0002991117653436959\n",
      "Episode 0, Step Loss: 0.00015769153833389282\n",
      "Episode 0, Step Loss: 0.0001565124694025144\n",
      "Episode 0, Step Loss: 0.00019959139171987772\n",
      "Episode 0, Step Loss: 0.00013859440514352173\n",
      "Episode 0, Step Loss: 0.0006882867310196161\n",
      "Episode 0, Step Loss: 0.00029127972084097564\n",
      "Episode 0, Step Loss: 0.00024863655562512577\n",
      "Episode 0, Step Loss: 0.0004322887980379164\n",
      "Episode 0, Step Loss: 0.0002969692286569625\n",
      "Episode 0, Step Loss: 0.00025479524629190564\n",
      "Episode 0, Step Loss: 0.0004955302574671805\n",
      "Episode 0, Step Loss: 0.00031767686596140265\n",
      "Episode 0, Step Loss: 0.0002143924357369542\n",
      "Episode 0, Step Loss: 0.00023960827093105763\n",
      "Episode 0, Step Loss: 0.00030366156715899706\n",
      "Episode 0, Step Loss: 0.00021253728482406586\n",
      "Episode 0, Step Loss: 0.00017021107487380505\n",
      "Episode 0, Step Loss: 0.00016973225865513086\n",
      "Episode 0, Step Loss: 0.00018071645172312856\n",
      "Episode 0, Step Loss: 0.0003930852690245956\n",
      "Episode 0, Step Loss: 0.0002895221405196935\n",
      "Episode 0, Step Loss: 0.00023342712665908039\n",
      "Episode 0, Step Loss: 0.0003619947237893939\n",
      "Episode 0, Step Loss: 0.00010350107186241075\n",
      "Episode 0, Step Loss: 0.00022916632588021457\n",
      "Episode 0, Step Loss: 0.0001931516599142924\n",
      "Episode 0, Step Loss: 0.00018502442981116474\n",
      "Episode 0, Step Loss: 0.0005040867254137993\n",
      "Episode 0, Step Loss: 0.00031034520361572504\n",
      "Episode 0, Step Loss: 0.00044598322710953653\n",
      "Episode 0, Step Loss: 0.00037055998109281063\n",
      "Episode 0, Step Loss: 0.0003668740391731262\n",
      "Episode 0, Step Loss: 0.00019235706713516265\n",
      "Episode 0, Step Loss: 0.00016129696450661868\n",
      "Episode 0, Step Loss: 0.00015290624287445098\n",
      "Episode 0, Step Loss: 0.0007318663992919028\n",
      "Episode 0, Step Loss: 0.00019779070862568915\n",
      "Episode 0, Step Loss: 0.00010894773004110903\n",
      "Episode 0, Step Loss: 0.00032078742515295744\n",
      "Episode 0, Step Loss: 0.00035051480517722666\n",
      "Episode 0, Step Loss: 0.00017357952310703695\n",
      "Episode 0, Step Loss: 0.0002690706169232726\n",
      "Episode 0, Step Loss: 0.0002776705950964242\n",
      "Episode 0, Step Loss: 0.000606200541369617\n",
      "Episode 0, Step Loss: 0.0003974043356720358\n",
      "Episode 0, Step Loss: 0.00012453518866095692\n",
      "Episode 0, Step Loss: 0.00030007457826286554\n",
      "Episode 0, Step Loss: 8.510189945809543e-05\n",
      "Episode 0, Step Loss: 0.00017682545876596123\n",
      "Episode 0, Step Loss: 7.079194620018825e-05\n",
      "Episode 0, Step Loss: 0.00019631459144875407\n",
      "Episode 0, Step Loss: 0.00034815273829735816\n",
      "Episode 0, Step Loss: 0.00018011601059697568\n",
      "Episode 0, Step Loss: 0.00048469126340933144\n",
      "Episode 0, Step Loss: 0.0002089471381623298\n",
      "Episode 0, Step Loss: 0.00014252564869821072\n",
      "Episode 0, Step Loss: 0.0005008658627048135\n",
      "Episode 0, Step Loss: 0.00030824021087028086\n",
      "Episode 0, Step Loss: 0.00027340283850207925\n",
      "Episode 0, Step Loss: 0.00037849717773497105\n",
      "Episode 0, Step Loss: 0.00021810925682075322\n",
      "Episode 0, Step Loss: 0.0001911564322654158\n",
      "Episode 0, Step Loss: 0.00025812399690039456\n",
      "Episode 0, Step Loss: 0.00011382578668417409\n",
      "Episode 0, Step Loss: 0.00019101981888525188\n",
      "Episode 0, Step Loss: 0.0005237663863226771\n",
      "Episode 0, Step Loss: 0.00035885165561921895\n",
      "Episode 0, Step Loss: 0.00016982447414193302\n",
      "Episode 0, Step Loss: 0.000355730764567852\n",
      "Episode 0, Step Loss: 0.000131556109408848\n",
      "Episode 0, Step Loss: 0.00030242837965488434\n",
      "Episode 0, Step Loss: 0.0002784907119348645\n",
      "Episode 0, Step Loss: 0.0005059123504906893\n",
      "Episode 0, Step Loss: 0.0003793216310441494\n",
      "Episode 0, Step Loss: 0.0008535314118489623\n",
      "Episode 0, Step Loss: 0.0005430640303529799\n",
      "Episode 0, Step Loss: 0.00022956386965233833\n",
      "Episode 0, Step Loss: 0.0002970240020658821\n",
      "Episode 0, Step Loss: 0.0006653824239037931\n",
      "Episode 0, Step Loss: 0.00021517908317036927\n",
      "Episode 0, Step Loss: 0.0002504202420823276\n",
      "Episode 0, Step Loss: 0.0002924372674897313\n",
      "Episode 0, Step Loss: 0.00013360512093640864\n",
      "Episode 0, Step Loss: 0.00041427474934607744\n",
      "Episode 0, Step Loss: 0.0002966912288684398\n",
      "Episode 0, Step Loss: 0.00014541132259182632\n",
      "Episode 0, Step Loss: 0.0002301283966517076\n",
      "Episode 0, Step Loss: 0.00013601782848127186\n",
      "Episode 0, Step Loss: 0.0003266470448579639\n",
      "Episode 0, Step Loss: 0.00035667791962623596\n",
      "Episode 0, Step Loss: 0.0006094661657698452\n",
      "Episode 0, Step Loss: 0.0003703822731040418\n",
      "Episode 0, Step Loss: 0.00021345267305150628\n",
      "Episode 0, Step Loss: 0.00012108093505958095\n",
      "Episode 0, Step Loss: 0.0002834378683473915\n",
      "Episode 0, Step Loss: 0.00031467786175198853\n",
      "Episode 0, Step Loss: 0.0001351601240457967\n",
      "Episode 0, Step Loss: 0.00024068796483334154\n",
      "Episode 0, Step Loss: 0.0002178034046664834\n",
      "Episode 0, Step Loss: 0.00040233173058368266\n",
      "Episode 0, Step Loss: 0.00022548432752955705\n",
      "Episode 0, Step Loss: 0.00020790065173059702\n",
      "Episode 0, Step Loss: 0.00033105580951087177\n",
      "Episode 0, Step Loss: 0.00030311819864436984\n",
      "Episode 0, Step Loss: 0.0003706142015289515\n",
      "Episode 0, Step Loss: 0.0005415189662016928\n",
      "Episode 0, Step Loss: 0.000270628253929317\n",
      "Episode 0, Step Loss: 0.0004765750782098621\n",
      "Episode 0, Step Loss: 0.0002031791373156011\n",
      "Episode 0, Step Loss: 0.00036096147960051894\n",
      "Episode 0, Step Loss: 0.0004657774115912616\n",
      "Episode 0, Step Loss: 0.0004330748342908919\n",
      "Episode 0, Step Loss: 0.00024283945094794035\n",
      "Episode 0, Step Loss: 0.00038225596654228866\n",
      "Episode 0, Step Loss: 0.0001601596741238609\n",
      "Episode 0, Step Loss: 0.00029682673630304635\n",
      "Episode 0, Step Loss: 0.0001212752831634134\n",
      "Episode 0, Step Loss: 0.0001628403115319088\n",
      "Episode 0, Step Loss: 0.0002472027263138443\n",
      "Episode 0, Step Loss: 0.0004402418853715062\n",
      "Episode 0, Step Loss: 0.0002953216608148068\n",
      "Episode 0, Step Loss: 0.0004312592791393399\n",
      "Episode 0, Step Loss: 0.00025078735779970884\n",
      "Episode 0, Step Loss: 0.00030187013908289373\n",
      "Episode 0, Step Loss: 0.00037712641642428935\n",
      "Episode 0, Step Loss: 0.0003651060105767101\n",
      "Episode 0, Step Loss: 0.0006418994744308293\n",
      "Episode 0, Step Loss: 0.0004131790774408728\n",
      "Episode 0, Step Loss: 0.00032293537515215576\n",
      "Episode 0, Step Loss: 0.0002570532087702304\n",
      "Episode 0, Step Loss: 0.0001183216372737661\n",
      "Episode 0, Step Loss: 0.000258042651694268\n",
      "Episode 0, Step Loss: 0.0004991673631593585\n",
      "Episode 0, Step Loss: 0.0005627741338685155\n",
      "Episode 0, Step Loss: 0.0003859323915094137\n",
      "Episode 0, Step Loss: 0.00015861850988585502\n",
      "Episode 0, Step Loss: 0.0002065709268208593\n",
      "Episode 0, Step Loss: 0.00043248647125437856\n",
      "Episode 0, Step Loss: 0.0002714245638344437\n",
      "Episode 0, Step Loss: 0.0006468313513323665\n",
      "Episode 0, Step Loss: 0.00039406283758580685\n",
      "Episode 0, Step Loss: 0.00020088860765099525\n",
      "Episode 0, Step Loss: 0.00027217197930440307\n",
      "Episode 0, Step Loss: 0.00025522735086269677\n",
      "Episode 0, Step Loss: 0.0003024508769158274\n",
      "Episode 0, Step Loss: 0.0002651526010595262\n",
      "Episode 0, Step Loss: 0.00020022407989017665\n",
      "Episode 0, Step Loss: 0.0002765045501291752\n",
      "Episode 0, Step Loss: 0.00014321271737571806\n",
      "Episode 0, Step Loss: 0.0001130056771216914\n",
      "Episode 0, Step Loss: 0.00016009174578357488\n",
      "Episode 0, Step Loss: 0.0002343999221920967\n",
      "Episode 0, Step Loss: 9.141024929704145e-05\n",
      "Episode 0, Step Loss: 0.0002194217377109453\n",
      "Episode 0, Step Loss: 0.00017425112309865654\n",
      "Episode 0, Step Loss: 0.00013131514424458146\n",
      "Episode 0, Step Loss: 0.00011351457942510024\n",
      "Episode 0, Step Loss: 0.00023413702729158103\n",
      "Episode 0, Step Loss: 0.00019965948013123125\n",
      "Episode 0, Step Loss: 0.0002830479934345931\n",
      "Episode 0, Step Loss: 0.00010519908391870558\n",
      "Episode 0, Step Loss: 0.0003225422988180071\n",
      "Episode 0, Step Loss: 0.0006264239200390875\n",
      "Episode 0, Step Loss: 0.00026953115593641996\n",
      "Episode 0, Step Loss: 0.00029736472060903907\n",
      "Episode 0, Step Loss: 0.0006506908684968948\n",
      "Episode 0, Step Loss: 0.0004082238010596484\n",
      "Episode 0, Step Loss: 0.00030645623337477446\n",
      "Episode 0, Step Loss: 0.0005365446559153497\n",
      "Episode 0, Step Loss: 0.0005551670328713953\n",
      "Episode 0, Step Loss: 0.000299235136481002\n",
      "Episode 0, Step Loss: 0.0001974816550500691\n",
      "Episode 0, Step Loss: 0.00030977692222222686\n",
      "Episode 0, Step Loss: 0.0002709515974856913\n",
      "Episode 0, Step Loss: 0.00012677126505877823\n",
      "Episode 0, Step Loss: 0.0005875909118913114\n",
      "Episode 0, Step Loss: 0.0003899316943716258\n",
      "Episode 0, Step Loss: 0.00014858854410704225\n",
      "Episode 0, Step Loss: 0.000252551311859861\n",
      "Episode 0, Step Loss: 0.00034663776750676334\n",
      "Episode 0, Step Loss: 0.00013933671289123595\n",
      "Episode 0, Step Loss: 0.0005904422141611576\n",
      "Episode 0, Step Loss: 9.998289897339419e-05\n",
      "Episode 0, Step Loss: 0.00020481576211750507\n",
      "Episode 0, Step Loss: 0.0002495693333912641\n",
      "Episode 0, Step Loss: 0.0003737708320841193\n",
      "Episode 0, Step Loss: 0.0005555720417760313\n",
      "Episode 0, Step Loss: 0.0003541404730640352\n",
      "Episode 0, Step Loss: 0.00023009342839941382\n",
      "Episode 0, Step Loss: 0.00015100902237463742\n",
      "Episode 0, Step Loss: 0.00026027896092273295\n",
      "Episode 0, Step Loss: 0.0001329032238572836\n",
      "Episode 0, Step Loss: 0.00028254915378056467\n",
      "Episode 0, Step Loss: 0.0007534826872870326\n",
      "Episode 0, Step Loss: 0.00038294700789265335\n",
      "Episode 0, Step Loss: 0.0002676913281902671\n",
      "Episode 0, Step Loss: 0.00012458885612431914\n",
      "Episode 0, Step Loss: 0.0001530103909317404\n",
      "Episode 0, Step Loss: 0.0004074922180734575\n",
      "Episode 0, Step Loss: 0.0003377182874828577\n",
      "Episode 0, Step Loss: 0.00022786867339164019\n",
      "Episode 0, Step Loss: 0.0001569529267726466\n",
      "Episode 0, Step Loss: 0.0002649958769325167\n",
      "Episode 0, Step Loss: 0.0004341293533798307\n",
      "Episode 0, Step Loss: 0.0002140034339390695\n",
      "Episode 0, Step Loss: 0.00014924476272426546\n",
      "Episode 0, Step Loss: 0.00021233495499473065\n",
      "Episode 0, Step Loss: 0.00036066563916392624\n",
      "Episode 0, Step Loss: 0.00018633107538335025\n",
      "Episode 0, Step Loss: 0.00021746914717368782\n",
      "Episode 0, Step Loss: 0.0002139304269803688\n",
      "Episode 0, Step Loss: 0.00011779415217461064\n",
      "Episode 0, Step Loss: 0.00014387795818038285\n",
      "Episode 0, Step Loss: 0.00020222619059495628\n",
      "Episode 0, Step Loss: 0.0002325951209058985\n",
      "Episode 0, Step Loss: 0.0003661811351776123\n",
      "Episode 0, Step Loss: 0.0001235363888554275\n",
      "Episode 0, Step Loss: 0.00013992794265504926\n",
      "Episode 0, Step Loss: 0.0002136552648153156\n",
      "Episode 0, Step Loss: 0.00017120855045504868\n",
      "Episode 0, Step Loss: 0.0002472129708621651\n",
      "Episode 0, Step Loss: 0.00014213686517905444\n",
      "Episode 0, Step Loss: 0.00039183194166980684\n",
      "Episode 0, Step Loss: 0.0004289122298359871\n",
      "Episode 0, Step Loss: 0.00022066869132686406\n",
      "Episode 0, Step Loss: 0.0002709541004151106\n",
      "Episode 0, Step Loss: 0.00038648172630928457\n",
      "Episode 0, Step Loss: 0.00037436062120832503\n",
      "Episode 0, Step Loss: 0.00020781508646905422\n",
      "Episode 0, Step Loss: 0.00012051689554937184\n",
      "Episode 0, Step Loss: 0.00023149031039793044\n",
      "Episode 0, Step Loss: 0.0004636266967281699\n",
      "Episode 0, Step Loss: 0.00029059615917503834\n",
      "Episode 0, Step Loss: 0.0003957892186008394\n",
      "Episode 0, Step Loss: 0.00018449631170369685\n",
      "Episode 0, Step Loss: 0.00018117259605787694\n",
      "Episode 0, Step Loss: 0.00045749550918117166\n",
      "Episode 0, Step Loss: 0.00012586750381160527\n",
      "Episode 0, Step Loss: 0.0002230190730188042\n",
      "Episode 0, Step Loss: 0.0002725006197579205\n",
      "Episode 0, Step Loss: 0.0006824859883636236\n",
      "Episode 0, Step Loss: 0.0006220166687853634\n",
      "Episode 0, Step Loss: 0.00021396410011220723\n",
      "Episode 0, Step Loss: 0.0001759554143063724\n",
      "Episode 0, Step Loss: 0.0004712780937552452\n",
      "Episode 0, Step Loss: 0.00016228501044679433\n",
      "Episode 0, Step Loss: 0.0003496784484013915\n",
      "Episode 0, Step Loss: 0.00021757904323749244\n",
      "Episode 0, Step Loss: 0.0001229894842253998\n",
      "Episode 0, Step Loss: 0.00018015332170762122\n",
      "Episode 0, Step Loss: 0.0001582747499924153\n",
      "Episode 0, Step Loss: 0.00034178083296865225\n",
      "Episode 0, Step Loss: 0.00019043087377212942\n",
      "Episode 0, Step Loss: 0.0002481257834006101\n",
      "Episode 0, Step Loss: 0.00043624057434499264\n",
      "Episode 0, Step Loss: 0.0003376226522959769\n",
      "Episode 0, Step Loss: 0.00018331981846131384\n",
      "Episode 0, Step Loss: 0.0002697678573895246\n",
      "Episode 0, Step Loss: 0.0003722842666320503\n",
      "Episode 0, Step Loss: 0.00028258704696781933\n",
      "Episode 0, Step Loss: 0.00026123470161110163\n",
      "Episode 0, Step Loss: 0.00015738447837065905\n",
      "Episode 0, Step Loss: 0.00021261468646116555\n",
      "Episode 0, Step Loss: 0.0004011916753370315\n",
      "Episode 0, Step Loss: 0.00018232107686344534\n",
      "Episode 0, Step Loss: 0.0001617738016648218\n",
      "Episode 0, Step Loss: 0.00011176395491929725\n",
      "Episode 0, Step Loss: 0.00022388486831914634\n",
      "Episode 0, Step Loss: 0.0001508995337644592\n",
      "Episode 0, Step Loss: 0.0002256811858387664\n",
      "Episode 0, Step Loss: 0.00046772632049396634\n",
      "Episode 0, Step Loss: 9.820455306908116e-05\n",
      "Episode 0, Step Loss: 9.037165727932006e-05\n",
      "Episode 0, Step Loss: 0.00018027141049969941\n",
      "Episode 0, Step Loss: 0.00012940500164404511\n",
      "Episode 0, Step Loss: 0.00034408827195875347\n",
      "Episode 0, Step Loss: 0.00023528026940766722\n",
      "Episode 0, Step Loss: 0.0001562863471917808\n",
      "Episode 0, Step Loss: 0.00024830273468978703\n",
      "Episode 0, Step Loss: 0.0002413700131000951\n",
      "Episode 0, Step Loss: 0.00019490843988023698\n",
      "Episode 0, Step Loss: 0.0001410110853612423\n",
      "Episode 0, Step Loss: 0.00014985495363362134\n",
      "Episode 0, Step Loss: 4.518746573012322e-05\n",
      "Episode 0, Step Loss: 0.00036054386873729527\n",
      "Episode 0, Step Loss: 0.00043659485527314246\n",
      "Episode 0, Step Loss: 0.00022537600307259709\n",
      "Episode 0, Step Loss: 0.0010303142480552197\n",
      "Episode 0, Step Loss: 0.0001924173120642081\n",
      "Episode 0, Step Loss: 0.00016205475549213588\n",
      "Episode 0, Step Loss: 0.000283681380096823\n",
      "Episode 0, Step Loss: 0.00042121863225474954\n",
      "Episode 0, Step Loss: 0.0003373044019099325\n",
      "Episode 0, Step Loss: 0.00021331945026759058\n",
      "Episode 0, Step Loss: 0.0005788932321593165\n",
      "Episode 0, Step Loss: 0.00016994468751363456\n",
      "Episode 0, Step Loss: 0.00018288163118995726\n",
      "Episode 0, Step Loss: 0.0003852525260299444\n",
      "Episode 0, Step Loss: 0.0006568935932591558\n",
      "Episode 0, Step Loss: 0.00022259255638346076\n",
      "Episode 0, Step Loss: 0.00026762488414533436\n",
      "Episode 0, Step Loss: 0.00024623621720820665\n",
      "Episode 0, Step Loss: 0.00011971275671385229\n",
      "Episode 0, Step Loss: 0.00014671808457933366\n",
      "Episode 0, Step Loss: 0.0002477983071003109\n",
      "Episode 0, Step Loss: 0.00022495964367408305\n",
      "Episode 0, Step Loss: 0.00022240623366087675\n",
      "Episode 0, Step Loss: 0.00022419870947487652\n",
      "Episode 0, Step Loss: 0.0003513961855787784\n",
      "Episode 0, Step Loss: 0.0006249454454518855\n",
      "Episode 0, Step Loss: 0.00027694483287632465\n",
      "Episode 0, Step Loss: 0.0001997182989725843\n",
      "Episode 0, Step Loss: 0.00035082505200989544\n",
      "Episode 0, Step Loss: 0.0008920107502490282\n",
      "Episode 0, Step Loss: 0.00017985395970754325\n",
      "Episode 0, Step Loss: 0.000322192907333374\n",
      "Episode 0, Step Loss: 0.0006883636815473437\n",
      "Episode 0, Step Loss: 0.0004685426829382777\n",
      "Episode 0, Step Loss: 0.00022809348593000323\n",
      "Episode 0, Step Loss: 0.0003975644358433783\n",
      "Episode 0, Step Loss: 0.00024169767857529223\n",
      "Episode 0, Step Loss: 0.0006491414969787002\n",
      "Episode 0, Step Loss: 0.0003852162335533649\n",
      "Episode 0, Step Loss: 0.0004983560647815466\n",
      "Episode 0, Step Loss: 0.0005576185067184269\n",
      "Episode 0, Step Loss: 0.000490045000333339\n",
      "Episode 0, Step Loss: 0.002092834562063217\n",
      "Episode 0, Step Loss: 0.00036894247750751674\n",
      "Episode 0, Step Loss: 0.0003508494410198182\n",
      "Episode 0, Step Loss: 0.00022101114154793322\n",
      "Episode 0, Step Loss: 0.00017482189286965877\n",
      "Episode 0, Step Loss: 0.00029806134989485145\n",
      "Episode 0, Step Loss: 0.0004444284422788769\n",
      "Episode 0, Step Loss: 0.0008418902289122343\n",
      "Episode 0, Step Loss: 0.0006470982334576547\n",
      "Episode 0, Step Loss: 0.00043119490146636963\n",
      "Episode 0, Step Loss: 0.00024306184786837548\n",
      "Episode 0, Step Loss: 0.0002557996194809675\n",
      "Episode 0, Step Loss: 0.0004617942904587835\n",
      "Episode 0, Step Loss: 0.00037868230720050633\n",
      "Episode 0, Step Loss: 0.0005353449378162622\n",
      "Episode 0, Step Loss: 0.0002351021976210177\n",
      "Episode 0, Step Loss: 0.0009010733338072896\n",
      "Episode 0, Step Loss: 0.0003645203833002597\n",
      "Episode 0, Step Loss: 0.00037277661613188684\n",
      "Episode 0, Step Loss: 0.0007212364580482244\n",
      "Episode 0, Step Loss: 0.0006360075785778463\n",
      "Episode 0, Step Loss: 0.0002845890703611076\n",
      "Episode 0, Step Loss: 0.00045720100752077997\n",
      "Episode 0, Step Loss: 0.00020782960928045213\n",
      "Episode 0, Step Loss: 0.00030149982194416225\n",
      "Episode 0, Step Loss: 0.0003592992143239826\n",
      "Episode 0, Step Loss: 0.0005761065403930843\n",
      "Episode 0, Step Loss: 0.00044081074884161353\n",
      "Episode 0, Step Loss: 0.00025193128385581076\n",
      "Episode 0, Step Loss: 0.0002458845265209675\n",
      "Episode 0, Step Loss: 0.00033689136034809053\n",
      "Episode 0, Step Loss: 0.0002950180205516517\n",
      "Episode 0, Step Loss: 0.0005695566651411355\n",
      "Episode 0, Step Loss: 0.00031179486541077495\n",
      "Episode 0, Step Loss: 0.0001741646119626239\n",
      "Episode 0, Step Loss: 0.00027659122133627534\n",
      "Episode 0, Step Loss: 0.0008216992719098926\n",
      "Episode 0, Step Loss: 0.00028849486261606216\n",
      "Episode 0, Step Loss: 0.0003194960008841008\n",
      "Episode 0, Step Loss: 0.00020703990594483912\n",
      "Episode 0, Step Loss: 0.0005670770769938827\n",
      "Episode 0, Step Loss: 0.00028487565577961504\n",
      "Episode 0, Step Loss: 0.000224027200601995\n",
      "Episode 0, Step Loss: 0.0004594858328346163\n",
      "Episode 0, Step Loss: 0.000206400960450992\n",
      "Episode 0, Step Loss: 0.0005255969590507448\n",
      "Episode 0, Step Loss: 0.0004459743504412472\n",
      "Episode 0, Step Loss: 0.00040080660255625844\n",
      "Episode 0, Step Loss: 0.0002330293646082282\n",
      "Episode 0, Step Loss: 0.0003772640193346888\n",
      "Episode 0, Step Loss: 0.00029994314536452293\n",
      "Episode 0, Step Loss: 0.00041215497185476124\n",
      "Episode 0, Step Loss: 0.0007095509208738804\n",
      "Episode 0, Step Loss: 0.0002446403668727726\n",
      "Episode 0, Step Loss: 0.0005452602636069059\n",
      "Episode 0, Step Loss: 0.0003344567376188934\n",
      "Episode 0, Step Loss: 0.0007656430825591087\n",
      "Episode 0, Step Loss: 0.0004111176240257919\n",
      "Episode 0, Step Loss: 0.00018831546185538173\n",
      "Episode 0, Step Loss: 0.0004933573654852808\n",
      "Episode 0, Step Loss: 0.00027702911756932735\n",
      "Episode 0, Step Loss: 0.00025176964118145406\n",
      "Episode 0, Step Loss: 0.0007312028901651502\n",
      "Episode 0, Step Loss: 0.0005507026799023151\n",
      "Episode 0, Step Loss: 0.00033574196277186275\n",
      "Episode 0, Step Loss: 0.0005438313819468021\n",
      "Episode 0, Step Loss: 0.0003541827609296888\n",
      "Episode 0, Step Loss: 0.000501921633258462\n",
      "Episode 0, Step Loss: 0.0004245726449880749\n",
      "Episode 0, Step Loss: 0.0006482919561676681\n",
      "Episode 0, Step Loss: 0.0012811515480279922\n",
      "Episode 0, Step Loss: 0.0008439102675765753\n",
      "Episode 0, Step Loss: 0.0003150753036607057\n",
      "Episode 0, Step Loss: 0.0003412156947888434\n",
      "Episode 0, Step Loss: 0.0006385924061760306\n",
      "Episode 0, Step Loss: 0.0005235013668425381\n",
      "Episode 0, Step Loss: 0.00047667702892795205\n",
      "Episode 0, Step Loss: 0.00017820601351559162\n",
      "Episode 0, Step Loss: 0.000425774953328073\n",
      "Episode 0, Step Loss: 0.000897929712664336\n",
      "Episode 0, Step Loss: 0.000619300757534802\n",
      "Episode 0, Step Loss: 0.00021968550572637469\n",
      "Episode 0, Step Loss: 0.0005899115931242704\n",
      "Episode 0, Step Loss: 0.00032977559021674097\n",
      "Episode 0, Step Loss: 0.00020819561905227602\n",
      "Episode 0, Step Loss: 0.00024527215282432735\n",
      "Episode 0, Step Loss: 0.00017770631529856473\n",
      "Episode 0, Step Loss: 0.00014661223394796252\n",
      "Episode 0, Step Loss: 0.00035359725006856024\n",
      "Episode 0, Step Loss: 0.00029812747379764915\n",
      "Episode 0, Step Loss: 0.0003049738588742912\n",
      "Episode 0, Step Loss: 0.00013823577319271863\n",
      "Episode 0, Step Loss: 0.0008780885254964232\n",
      "Episode 0, Step Loss: 0.0004948703572154045\n",
      "Episode 0, Step Loss: 0.0009145250078290701\n",
      "Episode 0, Step Loss: 0.0002471052575856447\n",
      "Episode 0, Step Loss: 0.00027034111553803086\n",
      "Episode 0, Step Loss: 0.00037019047886133194\n",
      "Episode 0, Step Loss: 0.0008678971789777279\n",
      "Episode 0, Step Loss: 0.000307632697513327\n",
      "Episode 0, Step Loss: 0.0007161919493228197\n",
      "Episode 0, Step Loss: 0.0002526732278056443\n",
      "Episode 0, Step Loss: 0.00026606814935803413\n",
      "Episode 0, Step Loss: 0.0012237937189638615\n",
      "Episode 0, Step Loss: 0.0002021752152359113\n",
      "Episode 0, Step Loss: 0.0004597323713824153\n",
      "Episode 0, Step Loss: 0.00046419684076681733\n",
      "Episode 0, Step Loss: 0.0004262131988070905\n",
      "Episode 0, Step Loss: 0.0004267582844477147\n",
      "Episode 0, Step Loss: 0.00029787395033054054\n",
      "Episode 0, Step Loss: 0.0006761678378097713\n",
      "Episode 0, Step Loss: 0.0006750758620910347\n",
      "Episode 0, Step Loss: 0.0006245146505534649\n",
      "Episode 0, Step Loss: 0.0006251446320675313\n",
      "Episode 0, Step Loss: 0.00046823828597553074\n",
      "Episode 0, Step Loss: 0.0002521724090911448\n",
      "Episode 0, Step Loss: 0.00028485149960033596\n",
      "Episode 0, Step Loss: 0.0011883227853104472\n",
      "Episode 0, Step Loss: 0.000152428459841758\n",
      "Episode 0, Step Loss: 0.00023930391762405634\n",
      "Episode 0, Step Loss: 0.000270427466602996\n",
      "Episode 0, Step Loss: 0.000347971566952765\n",
      "Episode 0, Step Loss: 0.00040450505912303925\n",
      "Episode 0, Step Loss: 0.0006796140223741531\n",
      "Episode 0, Step Loss: 0.0006266584387049079\n",
      "Episode 0, Step Loss: 0.0001861192286014557\n",
      "Episode 0, Step Loss: 0.0003118405875284225\n",
      "Episode 0, Step Loss: 0.0005280281184241176\n",
      "Episode 0, Step Loss: 0.0003215658653061837\n",
      "Episode 0, Step Loss: 0.00011134634405607358\n",
      "Episode 0, Step Loss: 0.0005807811976410449\n",
      "Episode 0, Step Loss: 0.0008021162357181311\n",
      "Episode 0, Step Loss: 0.0006153755821287632\n",
      "Episode 0, Step Loss: 0.00018397618259768933\n",
      "Episode 0, Step Loss: 0.00022070470731705427\n",
      "Episode 0, Step Loss: 0.00037908682134002447\n",
      "Episode 0, Step Loss: 0.00020474984194152057\n",
      "Episode 0, Step Loss: 0.00021853289217688143\n",
      "Episode 0, Step Loss: 0.0009844079613685608\n",
      "Episode 0, Step Loss: 0.0006435514660552144\n",
      "Episode 0, Step Loss: 0.0006136255105957389\n",
      "Episode 0, Step Loss: 0.00011211405217181891\n",
      "Episode 0, Step Loss: 0.00026782334316521883\n",
      "Episode 0, Step Loss: 0.0007766394992358983\n",
      "Episode 0, Step Loss: 0.00024409941397607327\n",
      "Episode 0, Step Loss: 0.00040382175939157605\n",
      "Episode 0, Step Loss: 0.0005311036366038024\n",
      "Episode 0, Step Loss: 0.00046562557690776885\n",
      "Episode 0, Step Loss: 0.0004886977840214968\n",
      "Episode 0, Step Loss: 0.00046177327749319375\n",
      "Episode 0, Step Loss: 0.00046178774209693074\n",
      "Episode 0, Step Loss: 0.00029336518491618335\n",
      "Episode 0, Step Loss: 0.0007766798371449113\n",
      "Episode 0, Step Loss: 0.00038623809814453125\n",
      "Episode 0, Step Loss: 0.0001760978630045429\n",
      "Episode 0, Step Loss: 0.0005686565418727696\n",
      "Episode 0, Step Loss: 0.0004371256218291819\n",
      "Episode 0, Step Loss: 0.0006162284989841282\n",
      "Episode 0, Step Loss: 0.000173014952451922\n",
      "Episode 0, Step Loss: 0.00035089568700641394\n",
      "Episode 0, Step Loss: 0.0006284973933361471\n",
      "Episode 0, Step Loss: 0.0005475777434185147\n",
      "Episode 0, Step Loss: 0.00041979397065006196\n",
      "Episode 0, Step Loss: 0.0005900644464418292\n",
      "Episode 0, Step Loss: 0.0002412867615930736\n",
      "Episode 0, Step Loss: 0.000548053823877126\n",
      "Episode 0, Step Loss: 0.00018896572873927653\n",
      "Episode 0, Step Loss: 0.0006751411128789186\n",
      "Episode 0, Step Loss: 0.0001964618859346956\n",
      "Episode 0, Step Loss: 0.0008720774203538895\n",
      "Episode 0, Step Loss: 0.00012793560745194554\n",
      "Episode 0, Step Loss: 0.0004449372354429215\n",
      "Episode 0, Step Loss: 0.0001765878841979429\n",
      "Episode 0, Step Loss: 0.00072591652860865\n",
      "Episode 0, Step Loss: 0.0003936152206733823\n",
      "Episode 0, Step Loss: 0.00029365753289312124\n",
      "Episode 0, Step Loss: 0.0005647916696034372\n",
      "Episode 0, Step Loss: 0.00018811578047461808\n",
      "Episode 0, Step Loss: 0.00026418789639137685\n",
      "Episode 0, Step Loss: 0.0006988124223425984\n",
      "Episode 0, Step Loss: 0.000400449032895267\n",
      "Episode 0, Step Loss: 0.0002701908815652132\n",
      "Episode 0, Step Loss: 0.00032191083300858736\n",
      "Episode 0, Step Loss: 0.0001386819640174508\n",
      "Episode 0, Step Loss: 0.0006353774224407971\n",
      "Episode 0, Step Loss: 0.00043121486669406295\n",
      "Episode 0, Step Loss: 0.00025584743707440794\n",
      "Episode 0, Step Loss: 0.0004437532043084502\n",
      "Episode 0, Step Loss: 0.0003440159489400685\n",
      "Episode 0, Step Loss: 0.00038447853876277804\n",
      "Episode 0, Step Loss: 0.00022228111629374325\n",
      "Episode 0, Step Loss: 0.0006438661948777735\n",
      "Episode 0, Step Loss: 0.00043438110151328146\n",
      "Episode 0, Step Loss: 0.0002655118005350232\n",
      "Episode 0, Step Loss: 0.0001964960974873975\n",
      "Episode 0, Step Loss: 0.0003104067873209715\n",
      "Episode 0, Step Loss: 0.0005362414522096515\n",
      "Episode 0, Step Loss: 0.0008666374487802386\n",
      "Episode 0, Step Loss: 0.00020173282246105373\n",
      "Episode 0, Step Loss: 0.000551852397620678\n",
      "Episode 0, Step Loss: 0.00036556378472596407\n",
      "Episode 0, Step Loss: 0.0003791450581047684\n",
      "Episode 0, Step Loss: 0.0003103706403635442\n",
      "Episode 0, Step Loss: 0.00020278809824958444\n",
      "Episode 0, Step Loss: 0.00048330173012800515\n",
      "Episode 0, Step Loss: 0.0003077466390095651\n",
      "Episode 0, Step Loss: 0.00020886144193354994\n",
      "Episode 0, Step Loss: 0.00013437619782052934\n",
      "Episode 0, Step Loss: 0.00013398012379184365\n",
      "Episode 0, Step Loss: 0.00021403383288998157\n",
      "Episode 0, Step Loss: 0.0003745371650438756\n",
      "Episode 0, Step Loss: 0.0006041752640157938\n",
      "Episode 0, Step Loss: 0.00019016218720935285\n",
      "Episode 0, Step Loss: 0.001358476816676557\n",
      "Episode 0, Step Loss: 0.0005661440081894398\n",
      "Episode 0, Step Loss: 0.0005819383077323437\n",
      "Episode 0, Step Loss: 0.00014899607049301267\n",
      "Episode 0, Step Loss: 0.00032877520425245166\n",
      "Episode 0, Step Loss: 0.0009297443320974708\n",
      "Episode 0, Step Loss: 0.0006324177957139909\n",
      "Episode 0, Step Loss: 0.00019442333723418415\n",
      "Episode 0, Step Loss: 0.0006804215954616666\n",
      "Episode 0, Step Loss: 0.00043100217590108514\n",
      "Episode 0, Step Loss: 0.0006875797989778221\n",
      "Episode 0, Step Loss: 0.0006300581735558808\n",
      "Episode 0, Step Loss: 0.0004908393020741642\n",
      "Episode 0, Step Loss: 0.00027005799347534776\n",
      "Episode 0, Step Loss: 0.00022296754468698055\n",
      "Episode 0, Step Loss: 0.0006452057859860361\n",
      "Episode 0, Step Loss: 0.00025409209774807096\n",
      "Episode 0, Step Loss: 0.0006934870034456253\n",
      "Episode 0, Step Loss: 0.00023403129307553172\n",
      "Episode 0, Step Loss: 0.0005624000914394855\n",
      "Episode 0, Step Loss: 0.0001974120386876166\n",
      "Episode 0, Step Loss: 0.0005385533440858126\n",
      "Episode 0, Step Loss: 0.0004498348571360111\n",
      "Episode 0, Step Loss: 0.000117316740215756\n",
      "Episode 0, Step Loss: 0.00016380564193241298\n",
      "Episode 0, Step Loss: 0.0006330730975605547\n",
      "Episode 0, Step Loss: 0.00010148349247174338\n",
      "Episode 0, Step Loss: 0.0005551980575546622\n",
      "Episode 0, Step Loss: 0.0004005173686891794\n",
      "Episode 0, Step Loss: 0.0005364259704947472\n",
      "Episode 0, Step Loss: 0.0004217779787722975\n",
      "Episode 0, Step Loss: 0.00015690055442973971\n",
      "Episode 0, Step Loss: 0.00012844157754443586\n",
      "Episode 0, Step Loss: 0.0002591166994534433\n",
      "Episode 0, Step Loss: 0.00037260670796968043\n",
      "Episode 0, Step Loss: 0.0003837707336060703\n",
      "Episode 0, Step Loss: 0.0005650902749039233\n",
      "Episode 0, Step Loss: 0.0001685271563474089\n",
      "Episode 0, Step Loss: 0.00044443958904594183\n",
      "Episode 0, Step Loss: 0.00022298812109511346\n",
      "Episode 0, Step Loss: 0.0010804751655086875\n",
      "Episode 0, Step Loss: 0.00038686010520905256\n",
      "Episode 0, Step Loss: 0.0005622684257104993\n",
      "Episode 0, Step Loss: 0.00026777462335303426\n",
      "Episode 0, Step Loss: 0.0005315022426657379\n",
      "Episode 0, Step Loss: 0.0007842844352126122\n",
      "Episode 0, Step Loss: 0.00042672338895499706\n",
      "Episode 0, Step Loss: 0.0005473954370245337\n",
      "Episode 0, Step Loss: 0.00029278674628585577\n",
      "Episode 0, Step Loss: 0.0003647694829851389\n",
      "Episode 0, Step Loss: 0.00047363212797790766\n",
      "Episode 0, Step Loss: 0.0008404784603044391\n",
      "Episode 0, Step Loss: 0.0002275778679177165\n",
      "Episode 0, Step Loss: 0.0005856252973899245\n",
      "Episode 0, Step Loss: 0.00024305674014613032\n",
      "Episode 0, Step Loss: 0.00046883191680535674\n",
      "Episode 0, Step Loss: 0.0003550670517142862\n",
      "Episode 0, Step Loss: 0.00028920319164171815\n",
      "Episode 0, Step Loss: 0.0002701687626540661\n",
      "Episode 0, Step Loss: 0.00032468681456521153\n",
      "Episode 0, Step Loss: 0.00041961969691328704\n",
      "Episode 0, Step Loss: 0.00025277951499447227\n",
      "Episode 0, Step Loss: 0.0005980818532407284\n",
      "Episode 0, Step Loss: 0.0007124916883185506\n",
      "Episode 0, Step Loss: 0.0005023130215704441\n",
      "Episode 0, Step Loss: 0.0005559074343182147\n",
      "Episode 0, Step Loss: 0.00027617899468168616\n",
      "Episode 0, Step Loss: 0.0001425242517143488\n",
      "Episode 0, Step Loss: 0.0004510782891884446\n",
      "Episode 0, Step Loss: 0.0003951778926420957\n",
      "Episode 0, Step Loss: 0.00020553932699840516\n",
      "Episode 0, Step Loss: 0.0001678894623182714\n",
      "Episode 0, Step Loss: 0.0004919831408187747\n",
      "Episode 0, Step Loss: 0.0006807147292420268\n",
      "Episode 0, Step Loss: 0.0003799045807681978\n",
      "Episode 0, Step Loss: 0.0007627121522091329\n",
      "Episode 0, Step Loss: 0.00040189677383750677\n",
      "Episode 0, Step Loss: 0.0005525153828784823\n",
      "Episode 0, Step Loss: 0.0001691233046585694\n",
      "Episode 0, Step Loss: 0.00027628339012153447\n",
      "Episode 0, Step Loss: 0.00019427888037171215\n",
      "Episode 0, Step Loss: 0.0008986559114418924\n",
      "Episode 0, Step Loss: 0.0003523034101817757\n",
      "Episode 0, Step Loss: 0.00014514678332488984\n",
      "Episode 0, Step Loss: 0.0005700613255612552\n",
      "Episode 0, Step Loss: 0.00019760039867833257\n",
      "Episode 0, Step Loss: 0.0002815252519212663\n",
      "Episode 0, Step Loss: 0.00017884904809761792\n",
      "Episode 0, Step Loss: 0.00032162026036530733\n",
      "Episode 0, Step Loss: 0.00020478293299674988\n",
      "Episode 0, Step Loss: 0.0003374164516571909\n",
      "Episode 0, Step Loss: 0.0008806320838630199\n",
      "Episode 0, Step Loss: 0.0007572822505608201\n",
      "Episode 0, Step Loss: 0.00016704134759493172\n",
      "Episode 0, Step Loss: 0.0002203822077717632\n",
      "Episode 0, Step Loss: 0.000213185980101116\n",
      "Episode 0, Step Loss: 0.0007087414851412177\n",
      "Episode 0, Step Loss: 0.0001762485917424783\n",
      "Episode 0, Step Loss: 0.00018277904018759727\n",
      "Episode 0, Step Loss: 0.00015670573338866234\n",
      "Episode 0, Step Loss: 0.0006260618101805449\n",
      "Episode 0, Step Loss: 0.0009598699398338795\n",
      "Episode 0, Step Loss: 0.0002307392715010792\n",
      "Episode 0, Step Loss: 0.00013427477097138762\n",
      "Episode 0, Step Loss: 0.0003207489789929241\n",
      "Episode 0, Step Loss: 0.00013027286331634969\n",
      "Episode 0, Step Loss: 0.0008987208711914718\n",
      "Episode 0, Step Loss: 0.00022798216377850622\n",
      "Episode 0, Step Loss: 0.0008427780121564865\n",
      "Episode 0, Step Loss: 0.0002134162641596049\n",
      "Episode 0, Step Loss: 0.0002915695949923247\n",
      "Episode 0, Step Loss: 0.0002838242216967046\n",
      "Episode 0, Step Loss: 0.00027390519971959293\n",
      "Episode 0, Step Loss: 0.0007503405795432627\n",
      "Episode 0, Step Loss: 0.00019553308084141463\n",
      "Episode 0, Step Loss: 0.00042440605466254056\n",
      "Episode 0, Step Loss: 0.0002755667082965374\n",
      "Episode 0, Step Loss: 0.00019644324493128806\n",
      "Episode 0, Step Loss: 0.0007259497651830316\n",
      "Episode 0, Step Loss: 0.0008042082190513611\n",
      "Episode 0, Step Loss: 0.0008617315907031298\n",
      "Episode 0, Step Loss: 0.0003737625665962696\n",
      "Episode 0, Step Loss: 0.00047270135837607086\n",
      "Episode 0, Step Loss: 0.00038857682375237346\n",
      "Episode 0, Step Loss: 0.0002577712293714285\n",
      "Episode 0, Step Loss: 0.00037635810440406203\n",
      "Episode 0, Step Loss: 0.0002659635210875422\n",
      "Episode 0, Step Loss: 0.00028771243523806334\n",
      "Episode 0, Step Loss: 0.0007941849180497229\n",
      "Episode 0, Step Loss: 0.0001121213281294331\n",
      "Episode 0, Step Loss: 0.0004428449901752174\n",
      "Episode 0, Step Loss: 0.00018858911062125117\n",
      "Episode 0, Step Loss: 0.00017971431952901185\n",
      "Episode 0, Step Loss: 0.0008491880726069212\n",
      "Episode 0, Step Loss: 0.00017568646580912173\n",
      "Episode 0, Step Loss: 0.00045422595576383173\n",
      "Episode 0, Step Loss: 0.00039882323471829295\n",
      "Episode 0, Step Loss: 0.0006004210445098579\n",
      "Episode 0, Step Loss: 0.000609701732173562\n",
      "Episode 0, Step Loss: 0.000618007208686322\n",
      "Episode 0, Step Loss: 0.000855972757562995\n",
      "Episode 0, Step Loss: 0.001115260412916541\n",
      "Episode 0, Step Loss: 0.0003980082110501826\n",
      "Episode 0, Step Loss: 0.0002562006702646613\n",
      "Episode 0, Step Loss: 0.000407468993216753\n",
      "Episode 0, Step Loss: 0.00018932254170067608\n",
      "Episode 0, Step Loss: 0.00032287463545799255\n",
      "Episode 0, Step Loss: 0.00022318719129543751\n",
      "Episode 0, Step Loss: 0.00027529956423677504\n",
      "Episode 0, Step Loss: 0.00022041695774532855\n",
      "Episode 0, Step Loss: 0.0002702722849790007\n",
      "Episode 0, Step Loss: 0.00016433527343906462\n",
      "Episode 0, Step Loss: 0.00023462816898245364\n",
      "Episode 0, Step Loss: 0.0005416857311502099\n",
      "Episode 0, Step Loss: 0.0005350056453607976\n",
      "Episode 0, Step Loss: 0.0006174111040309072\n",
      "Episode 0, Step Loss: 0.00035247858613729477\n",
      "Episode 0, Step Loss: 0.0008812558953650296\n",
      "Episode 0, Step Loss: 0.0003072159888688475\n",
      "Episode 0, Step Loss: 0.00018123775953426957\n",
      "Episode 0, Step Loss: 8.9635796030052e-05\n",
      "Episode 0, Step Loss: 0.000254310667514801\n",
      "Episode 0, Step Loss: 0.0006406964967027307\n",
      "Episode 0, Step Loss: 0.000244208553340286\n",
      "Episode 0, Step Loss: 0.00045964523451402783\n",
      "Episode 0, Step Loss: 0.0005586596089415252\n",
      "Episode 0, Step Loss: 0.0003232292365282774\n",
      "Episode 0, Step Loss: 0.00019738046103157103\n",
      "Episode 0, Step Loss: 0.0008805511170066893\n",
      "Episode 0, Step Loss: 0.00018464270397089422\n",
      "Episode 0, Step Loss: 0.00018915400141850114\n",
      "Episode 0, Step Loss: 0.00030355778289958835\n",
      "Episode 0, Step Loss: 0.000586966925766319\n",
      "Episode 0, Step Loss: 0.00037938577588647604\n",
      "Episode 0, Step Loss: 0.0003326757869217545\n",
      "Episode 0, Step Loss: 0.00024749236763454974\n",
      "Episode 0, Step Loss: 0.00020268824300728738\n",
      "Episode 0, Step Loss: 0.0001526019477751106\n",
      "Episode 0, Step Loss: 0.00040623999666422606\n",
      "Episode 0, Step Loss: 0.0002791597507894039\n",
      "Episode 0, Step Loss: 0.0004876401217188686\n",
      "Episode 0, Step Loss: 0.00019495878950692713\n",
      "Episode 0, Step Loss: 0.0003372264327481389\n",
      "Episode 0, Step Loss: 0.0003266509738750756\n",
      "Episode 0, Step Loss: 0.0005740331253036857\n",
      "Episode 0, Step Loss: 0.0005666858050972223\n",
      "Episode 0, Step Loss: 0.0001626138691790402\n",
      "Episode 0, Step Loss: 0.0005173954414203763\n",
      "Episode 0, Step Loss: 0.00045725906966254115\n",
      "Episode 0, Step Loss: 0.00039458757964894176\n",
      "Episode 0, Step Loss: 0.00048090238124132156\n",
      "Episode 0, Step Loss: 0.000483829528093338\n",
      "Episode 0, Step Loss: 0.0007708421908318996\n",
      "Episode 0, Step Loss: 0.0002382932580076158\n",
      "Episode 0, Step Loss: 0.0007646026788279414\n",
      "Episode 0, Step Loss: 0.0005078766844235361\n",
      "Episode 0, Step Loss: 0.00023812988365534693\n",
      "Episode 0, Step Loss: 0.0004082145169377327\n",
      "Episode 0, Step Loss: 0.0004469447594601661\n",
      "Episode 0, Step Loss: 0.0007322060992009938\n",
      "Episode 0, Step Loss: 0.0007889508851803839\n",
      "Episode 0, Step Loss: 0.0005855249473825097\n",
      "Episode 0, Step Loss: 0.0008004618575796485\n",
      "Episode 0, Step Loss: 0.0004550698504317552\n",
      "Episode 0, Step Loss: 0.00018349068704992533\n",
      "Episode 0, Step Loss: 0.00024273946473840624\n",
      "Episode 0, Step Loss: 0.00023355908342637122\n",
      "Episode 0, Step Loss: 0.00039760975050739944\n",
      "Episode 0, Step Loss: 0.00030707792029716074\n",
      "Episode 0, Step Loss: 0.0004133802140131593\n",
      "Episode 0, Step Loss: 0.00011561341671040282\n",
      "Episode 0, Step Loss: 0.0006971475086174905\n",
      "Episode 0, Step Loss: 0.00017326192755717784\n",
      "Episode 0, Step Loss: 0.00020179794228170067\n",
      "Episode 0, Step Loss: 0.000676165334880352\n",
      "Episode 0, Step Loss: 0.000688187952619046\n",
      "Episode 0, Step Loss: 0.0002756059402599931\n",
      "Episode 0, Step Loss: 0.0009712025639601052\n",
      "Episode 0, Step Loss: 0.00047342770267277956\n",
      "Episode 0, Step Loss: 0.0005542020662687719\n",
      "Episode 0, Step Loss: 0.00029615702806040645\n",
      "Episode 0, Step Loss: 0.0003010112268384546\n",
      "Episode 0, Step Loss: 0.0004955213516950607\n",
      "Episode 0, Step Loss: 0.00048548297490924597\n",
      "Episode 0, Step Loss: 0.0003655326727312058\n",
      "Episode 0, Step Loss: 0.0006511821411550045\n",
      "Episode 0, Step Loss: 0.0006233321619220078\n",
      "Episode 0, Step Loss: 0.000585103640332818\n",
      "Episode 0, Step Loss: 0.0004471179854590446\n",
      "Episode 0, Step Loss: 0.00010720949649112299\n",
      "Episode 0, Step Loss: 0.001034930581226945\n",
      "Episode 0, Step Loss: 0.00019231013720855117\n",
      "Episode 0, Step Loss: 0.0005597340641543269\n",
      "Episode 0, Step Loss: 9.401245188200846e-05\n",
      "Episode 0, Step Loss: 0.00048823977704159915\n",
      "Episode 0, Step Loss: 0.0008906739531084895\n",
      "Episode 0, Step Loss: 0.00021499594731722027\n",
      "Episode 0, Step Loss: 0.0008508603787049651\n",
      "Episode 0, Step Loss: 0.0007324762409552932\n",
      "Episode 0, Step Loss: 0.00028655343339778483\n",
      "Episode 0, Step Loss: 0.0005452273762784898\n",
      "Episode 0, Step Loss: 0.0003788300382439047\n",
      "Episode 0, Step Loss: 0.0004132406902499497\n",
      "Episode 0, Step Loss: 0.00018951909441966563\n",
      "Episode 0, Step Loss: 0.0006420424906536937\n",
      "Episode 0, Step Loss: 0.0005381318624131382\n",
      "Episode 0, Step Loss: 0.00029774662107229233\n",
      "Episode 0, Step Loss: 0.000557770486921072\n",
      "Episode 0, Step Loss: 0.0005830673617310822\n",
      "Episode 0, Step Loss: 0.0002323348307982087\n",
      "Episode 0, Step Loss: 0.0001854090514825657\n",
      "Episode 0, Step Loss: 0.0008032121113501489\n",
      "Episode 0, Step Loss: 0.00020459745428524911\n",
      "Episode 0, Step Loss: 0.0006997741875238717\n",
      "Episode 0, Step Loss: 0.0007929634884931147\n",
      "Episode 0, Step Loss: 0.0003055355336982757\n",
      "Episode 0, Step Loss: 0.00025071005802601576\n",
      "Episode 0, Step Loss: 0.0009065740741789341\n",
      "Episode 0, Step Loss: 0.000272698060143739\n",
      "Episode 0, Step Loss: 0.00044125685235485435\n",
      "Episode 0, Step Loss: 0.0001802600163500756\n",
      "Episode 0, Step Loss: 0.0003004682366736233\n",
      "Episode 0, Step Loss: 0.0004950474249199033\n",
      "Episode 0, Step Loss: 0.00022347171034198254\n",
      "Episode 0, Step Loss: 0.00030993224936537445\n",
      "Episode 0, Step Loss: 0.00046548660611733794\n",
      "Episode 0, Step Loss: 0.0005755440797656775\n",
      "Episode 0, Step Loss: 0.00011623819591477513\n",
      "Episode 0, Step Loss: 0.0009338510571978986\n",
      "Episode 0, Step Loss: 0.00012342935951892287\n",
      "Episode 0, Step Loss: 0.0005671369726769626\n",
      "Episode 0, Step Loss: 0.00043099684990011156\n",
      "Episode 0, Step Loss: 0.00041543113184161484\n",
      "Episode 0, Step Loss: 0.0006372851203195751\n",
      "Episode 0, Step Loss: 0.0001752246025716886\n",
      "Episode 0, Step Loss: 0.0002711545384954661\n",
      "Episode 0, Step Loss: 0.0006860433495603502\n",
      "Episode 0, Step Loss: 0.0006117920274846256\n",
      "Episode 0, Step Loss: 0.00043365865712985396\n",
      "Episode 0, Step Loss: 0.0005776406032964587\n",
      "Episode 0, Step Loss: 0.0004931359435431659\n",
      "Episode 0, Step Loss: 0.0005579760181717575\n",
      "Episode 0, Step Loss: 0.0006572245038114488\n",
      "Episode 0, Step Loss: 0.001653524348512292\n",
      "Episode 0, Step Loss: 0.00034582949592731893\n",
      "Episode 0, Step Loss: 0.0005482255364768207\n",
      "Episode 0, Step Loss: 0.0003958159068133682\n",
      "Episode 0, Step Loss: 0.0002810004516504705\n",
      "Episode 0, Step Loss: 0.0007183379493653774\n",
      "Episode 0, Step Loss: 0.0009881926234811544\n",
      "Episode 0, Step Loss: 0.0004732044180855155\n",
      "Episode 0, Step Loss: 0.0003561805351637304\n",
      "Episode 0, Step Loss: 0.0005482498672790825\n",
      "Episode 0, Step Loss: 0.00024542739265598357\n",
      "Episode 0, Step Loss: 0.0001417706662323326\n",
      "Episode 0, Step Loss: 0.0002715997106861323\n",
      "Episode 0, Step Loss: 0.00037186878034844995\n",
      "Episode 0, Step Loss: 0.00012622166832443327\n",
      "Episode 0, Step Loss: 0.0005239060847088695\n",
      "Episode 0, Step Loss: 0.0005839947843924165\n",
      "Episode 0, Step Loss: 0.0002144494210369885\n",
      "Episode 0, Step Loss: 0.0006198293413035572\n",
      "Episode 0, Step Loss: 0.00035489286528900266\n",
      "Episode 0, Step Loss: 0.0001725164329400286\n",
      "Episode 0, Step Loss: 0.00033605826320126653\n",
      "Episode 0, Step Loss: 0.0005173175595700741\n",
      "Episode 0, Step Loss: 0.00041951396269723773\n",
      "Episode 0, Step Loss: 0.00011606790212681517\n",
      "Episode 0, Step Loss: 0.0002488823374733329\n",
      "Episode 0, Step Loss: 0.00018016468675341457\n",
      "Episode 0, Step Loss: 0.0005399829242378473\n",
      "Episode 0, Step Loss: 0.00031419581500813365\n",
      "Episode 0, Step Loss: 0.0009152806596830487\n",
      "Episode 0, Step Loss: 0.0006953873089514673\n",
      "Episode 0, Step Loss: 0.00028670692699961364\n",
      "Episode 0, Step Loss: 0.00016808143118396401\n",
      "Episode 0, Step Loss: 0.00018324179109185934\n",
      "Episode 0, Step Loss: 0.00037535984301939607\n",
      "Episode 0, Step Loss: 0.0003383090952411294\n",
      "Episode 0, Step Loss: 0.000454103690572083\n",
      "Episode 0, Step Loss: 0.00020716415019705892\n",
      "Episode 0, Step Loss: 0.00047154573258012533\n",
      "Episode 0, Step Loss: 0.00047111307503655553\n",
      "Episode 0, Step Loss: 0.00021523011673707515\n",
      "Episode 0, Step Loss: 0.0008127518813125789\n",
      "Episode 0, Step Loss: 0.00036956355324946344\n",
      "Episode 0, Step Loss: 0.0001468406553613022\n",
      "Episode 0, Step Loss: 0.0001960390800377354\n",
      "Episode 0, Step Loss: 0.0002817892818711698\n",
      "Episode 0, Step Loss: 0.00042603950714692473\n",
      "Episode 0, Step Loss: 0.00020984970615245402\n",
      "Episode 0, Step Loss: 0.00039045256562530994\n",
      "Episode 0, Step Loss: 0.00020314405264798552\n",
      "Episode 0, Step Loss: 0.00020669001969508827\n",
      "Episode 0, Step Loss: 0.00023233043611980975\n",
      "Episode 0, Step Loss: 0.00018420314881950617\n",
      "Episode 0, Step Loss: 0.0004507070407271385\n",
      "Episode 0, Step Loss: 0.00022636020730715245\n",
      "Episode 0, Step Loss: 0.00032935960916802287\n",
      "Episode 0, Step Loss: 0.0005204981425777078\n",
      "Episode 0, Step Loss: 0.00020151374337729067\n",
      "Episode 0, Step Loss: 0.0003763585118576884\n",
      "Episode 0, Step Loss: 0.00021080505393911153\n",
      "Episode 0, Step Loss: 0.0003423425368964672\n",
      "Episode 0, Step Loss: 0.0003994162252638489\n",
      "Episode 0, Step Loss: 0.00010046016541309655\n",
      "Episode 0, Step Loss: 0.00011834433826152235\n",
      "Episode 0, Step Loss: 0.00022752315271645784\n",
      "Episode 0, Step Loss: 0.0006121798069216311\n",
      "Episode 0, Step Loss: 0.0005979211418889463\n",
      "Episode 0, Step Loss: 0.00034538182080723345\n",
      "Episode 0, Step Loss: 0.00015932571841403842\n",
      "Episode 0, Step Loss: 0.00020872161258012056\n",
      "Episode 0, Step Loss: 0.00020899588707834482\n",
      "Episode 0, Step Loss: 0.00033028257894329727\n",
      "Episode 0, Step Loss: 0.00022863487538415939\n",
      "Episode 0, Step Loss: 0.00049454381223768\n",
      "Episode 0, Step Loss: 0.0001362970215268433\n",
      "Episode 0, Step Loss: 0.00025279639521613717\n",
      "Episode 0, Step Loss: 0.0006770368199795485\n",
      "Episode 0, Step Loss: 0.0007788853254169226\n",
      "Episode 0, Step Loss: 0.00024975163978524506\n",
      "Episode 0, Step Loss: 0.00037323677679523826\n",
      "Episode 0, Step Loss: 0.0001910392747959122\n",
      "Episode 0, Step Loss: 0.0001260584976989776\n",
      "Episode 0, Step Loss: 0.0005958742694929242\n",
      "Episode 0, Step Loss: 0.0003306710859760642\n",
      "Episode 0, Step Loss: 0.0002322498185094446\n",
      "Episode 0, Step Loss: 0.00046877379645593464\n",
      "Episode 0, Step Loss: 0.00028821840533055365\n",
      "Episode 0, Step Loss: 0.00036981780431233346\n",
      "Episode 0, Step Loss: 0.00020032591419294477\n",
      "Episode 0, Step Loss: 0.0006340336403809488\n",
      "Episode 0, Step Loss: 0.0006939773447811604\n",
      "Episode 0, Step Loss: 0.0003842693113256246\n",
      "Episode 0, Step Loss: 0.000271974946372211\n",
      "Episode 0, Step Loss: 0.00015642907237634063\n",
      "Episode 0, Step Loss: 0.00033798927324824035\n",
      "Episode 0, Step Loss: 0.0001367787190247327\n",
      "Episode 0, Step Loss: 8.954992517828941e-05\n",
      "Episode 0, Step Loss: 0.0003422925074119121\n",
      "Episode 0, Step Loss: 0.0003915507404599339\n",
      "Episode 0, Step Loss: 0.0002105848107021302\n",
      "Episode 0, Step Loss: 0.0008979435078799725\n",
      "Episode 0, Step Loss: 0.00021672251750715077\n",
      "Episode 0, Step Loss: 0.0002356590994168073\n",
      "Episode 0, Step Loss: 0.0004109583096578717\n",
      "Episode 0, Step Loss: 0.00066798907937482\n",
      "Episode 0, Step Loss: 0.0004863997455686331\n",
      "Episode 0, Step Loss: 0.0002526641183067113\n",
      "Episode 0, Step Loss: 0.00017519344692118466\n",
      "Episode 0, Step Loss: 0.0010094235185533762\n",
      "Episode 0, Step Loss: 0.0005599177093245089\n",
      "Episode 0, Step Loss: 0.00012288046127650887\n",
      "Episode 0, Step Loss: 0.0003183478256687522\n",
      "Episode 0, Step Loss: 0.00045811536256223917\n",
      "Episode 0, Step Loss: 0.0005920747644267976\n",
      "Episode 0, Step Loss: 0.00011712631385307759\n",
      "Episode 0, Step Loss: 0.0009877000702545047\n",
      "Episode 0, Step Loss: 0.0002482965064700693\n",
      "Episode 0, Step Loss: 0.0002680533507373184\n",
      "Episode 0, Step Loss: 0.0008905488066375256\n",
      "Episode 0, Step Loss: 0.0004969586152583361\n",
      "Episode 0, Step Loss: 0.00025686260778456926\n",
      "Episode 0, Step Loss: 0.0006345001747831702\n",
      "Episode 0, Step Loss: 0.0004464249068405479\n",
      "Episode 0, Step Loss: 0.0007636240334250033\n",
      "Episode 0, Step Loss: 0.0006675762706436217\n",
      "Episode 0, Step Loss: 0.0001629467005841434\n",
      "Episode 0, Step Loss: 0.0003742585831787437\n",
      "Episode 0, Step Loss: 0.000464201788417995\n",
      "Episode 0, Step Loss: 0.00020039366791024804\n",
      "Episode 0, Step Loss: 0.0001556181232444942\n",
      "Episode 0, Step Loss: 0.0003828115586657077\n",
      "Episode 0, Step Loss: 0.00022420882305596024\n",
      "Episode 0, Step Loss: 0.00015930439985822886\n",
      "Episode 0, Step Loss: 0.0006460092263296247\n",
      "Episode 0, Step Loss: 0.0006232812302187085\n",
      "Episode 0, Step Loss: 0.0001970863959286362\n",
      "Episode 0, Step Loss: 0.0003210913564544171\n",
      "Episode 0, Step Loss: 0.00016566480917390436\n",
      "Episode 0, Step Loss: 0.0005639742012135684\n",
      "Episode 0, Step Loss: 0.00019274478836450726\n",
      "Episode 0, Step Loss: 0.0005062702693976462\n",
      "Episode 0, Step Loss: 0.000923192361369729\n",
      "Episode 0, Step Loss: 0.0003216691256966442\n",
      "Episode 0, Step Loss: 0.00021283089881762862\n",
      "Episode 0, Step Loss: 0.0004979753866791725\n",
      "Episode 0, Step Loss: 0.0006433155504055321\n",
      "Episode 0, Step Loss: 0.00042717490578070283\n",
      "Episode 0, Step Loss: 0.0005125259631313384\n",
      "Episode 0, Step Loss: 0.00021355172793846577\n",
      "Episode 0, Step Loss: 0.00043838933925144374\n",
      "Episode 0, Step Loss: 0.0001577726798132062\n",
      "Episode 0, Step Loss: 0.0005984315648674965\n",
      "Episode 0, Step Loss: 0.00023183728626463562\n",
      "Episode 0, Step Loss: 0.0006608158000744879\n",
      "Episode 0, Step Loss: 0.00011249088129261509\n",
      "Episode 0, Step Loss: 0.00020541709091048688\n",
      "Episode 0, Step Loss: 0.00055930414237082\n",
      "Episode 0, Step Loss: 0.00039905463927425444\n",
      "Episode 0, Step Loss: 0.0002196558052673936\n",
      "Episode 0, Step Loss: 0.0001281418080907315\n",
      "Episode 0, Step Loss: 0.00034046475775539875\n",
      "Episode 0, Step Loss: 0.0002742891665548086\n",
      "Episode 0, Step Loss: 0.00022230151807889342\n",
      "Episode 0, Step Loss: 0.0002536984393373132\n",
      "Episode 0, Step Loss: 0.0001592731277924031\n",
      "Episode 0, Step Loss: 0.0005735167069360614\n",
      "Episode 0, Step Loss: 0.00019685595179907978\n",
      "Episode 0, Step Loss: 0.00013552950986195356\n",
      "Episode 0, Step Loss: 0.00024310636217705905\n",
      "Episode 0, Step Loss: 0.0002720121992751956\n",
      "Episode 0, Step Loss: 0.0006833667866885662\n",
      "Episode 0, Step Loss: 0.0002475720248185098\n",
      "Episode 0, Step Loss: 0.00031737686367705464\n",
      "Episode 0, Step Loss: 0.00028728379402309656\n",
      "Episode 0, Step Loss: 0.00037381419679149985\n",
      "Episode 0, Step Loss: 0.00017793859296943992\n",
      "Episode 0, Step Loss: 0.0005108636105433106\n",
      "Episode 0, Step Loss: 0.0003360499977134168\n",
      "Episode 0, Step Loss: 0.00022494755103252828\n",
      "Episode 0, Step Loss: 0.000360121950507164\n",
      "Episode 0, Step Loss: 0.00020785018568858504\n",
      "Episode 0, Step Loss: 0.00031914410647004843\n",
      "Episode 0, Step Loss: 0.0005706656374968588\n",
      "Episode 0, Step Loss: 0.00020275189308449626\n",
      "Episode 0, Step Loss: 6.584137736354023e-05\n",
      "Episode 0, Step Loss: 0.0004119299992453307\n",
      "Episode 0, Step Loss: 0.00019810051890090108\n",
      "Episode 0, Step Loss: 0.00018103074398823082\n",
      "Episode 0, Step Loss: 0.0003521305916365236\n",
      "Episode 0, Step Loss: 0.00013827600923832506\n",
      "Episode 0, Step Loss: 0.00033277529291808605\n",
      "Episode 0, Step Loss: 0.0002762062067631632\n",
      "Episode 0, Step Loss: 0.00022311021166387945\n",
      "Episode 0, Step Loss: 0.000882425345480442\n",
      "Episode 0, Step Loss: 0.0002769114507827908\n",
      "Episode 0, Step Loss: 0.0005574113456532359\n",
      "Episode 0, Step Loss: 0.00034682254772633314\n",
      "Episode 0, Step Loss: 0.00047450713464058936\n",
      "Episode 0, Step Loss: 0.0007640185067430139\n",
      "Episode 0, Step Loss: 0.0003648759739007801\n",
      "Episode 0, Step Loss: 0.0005825160769745708\n",
      "Episode 0, Step Loss: 0.0002996536495629698\n",
      "Episode 0, Step Loss: 0.0002985933388117701\n",
      "Episode 0, Step Loss: 0.0003335430519655347\n",
      "Episode 0, Step Loss: 0.0007702789735049009\n",
      "Episode 0, Step Loss: 0.00030578268342651427\n",
      "Episode 0, Step Loss: 0.00021030157222412527\n",
      "Episode 0, Step Loss: 0.00032156784436665475\n",
      "Episode 0, Step Loss: 0.0004068305715918541\n",
      "Episode 0, Step Loss: 0.0004888451076112688\n",
      "Episode 0, Step Loss: 0.0005134674138389528\n",
      "Episode 0, Step Loss: 0.0006314622005447745\n",
      "Episode 0, Step Loss: 0.00018920618458651006\n",
      "Episode 0, Step Loss: 0.0002443718258291483\n",
      "Episode 0, Step Loss: 0.00041970302117988467\n",
      "Episode 0, Step Loss: 0.0005823265528306365\n",
      "Episode 0, Step Loss: 0.000427597580710426\n",
      "Episode 0, Step Loss: 0.0002775807515718043\n",
      "Episode 0, Step Loss: 0.000673378468491137\n",
      "Episode 0, Step Loss: 0.0005185747286304832\n",
      "Episode 0, Step Loss: 0.0004632683703675866\n",
      "Episode 0, Step Loss: 0.00013239943655207753\n",
      "Episode 0, Step Loss: 0.00025656152865849435\n",
      "Episode 0, Step Loss: 0.0006575323641300201\n",
      "Episode 0, Step Loss: 0.00022100949718151242\n",
      "Episode 0, Step Loss: 0.00019776599947363138\n",
      "Episode 0, Step Loss: 0.00026541907573118806\n",
      "Episode 0, Step Loss: 0.0005387450801208615\n",
      "Episode 0, Step Loss: 0.0010941866785287857\n",
      "Episode 0, Step Loss: 0.00021897970873396844\n",
      "Episode 0, Step Loss: 0.0007429355755448341\n",
      "Episode 0, Step Loss: 0.00017373549053445458\n",
      "Episode 0, Step Loss: 0.00016325543401762843\n",
      "Episode 0, Step Loss: 0.0002338574850000441\n",
      "Episode 0, Step Loss: 0.00031664391281083226\n",
      "Episode 0, Step Loss: 0.00013799758744426072\n",
      "Episode 0, Step Loss: 0.0003347590391058475\n",
      "Episode 0, Step Loss: 0.0002932375355158001\n",
      "Episode 0, Step Loss: 0.00023235184198711067\n",
      "Episode 0, Step Loss: 0.00032272469252347946\n",
      "Episode 0, Step Loss: 0.0006731998873874545\n",
      "Episode 0, Step Loss: 0.0004474960151128471\n",
      "Episode 0, Step Loss: 0.0007389108650386333\n",
      "Episode 0, Step Loss: 0.00014243418991100043\n",
      "Episode 0, Step Loss: 0.00040588731644675136\n",
      "Episode 0, Step Loss: 0.0008984403684735298\n",
      "Episode 0, Step Loss: 0.00048333429731428623\n",
      "Episode 0, Step Loss: 0.0006538298330269754\n",
      "Episode 0, Step Loss: 0.0003628970589488745\n",
      "Episode 0, Step Loss: 0.000303248263662681\n",
      "Episode 0, Step Loss: 0.0005729793338105083\n",
      "Episode 0, Step Loss: 0.0006012155790813267\n",
      "Episode 0, Step Loss: 0.00021930906223133206\n",
      "Episode 0, Step Loss: 0.00033640197943896055\n",
      "Episode 0, Step Loss: 0.00024760421365499496\n",
      "Episode 0, Step Loss: 0.00030255079036578536\n",
      "Episode 0, Step Loss: 0.0002413260517641902\n",
      "Episode 0, Step Loss: 0.00034871805110014975\n",
      "Episode 0, Step Loss: 0.00037827392225153744\n",
      "Episode 0, Step Loss: 0.00011455324420239776\n",
      "Episode 0, Step Loss: 0.0006080332095734775\n",
      "Episode 0, Step Loss: 0.0004955669865012169\n",
      "Episode 0, Step Loss: 0.00019723756122402847\n",
      "Episode 0, Step Loss: 0.0003830247442238033\n",
      "Episode 0, Step Loss: 0.0006502137403003871\n",
      "Episode 0, Step Loss: 0.0001861614582594484\n",
      "Episode 0, Step Loss: 0.00031666256836615503\n",
      "Episode 0, Step Loss: 0.0002623266482260078\n",
      "Episode 0, Step Loss: 0.00040356258978135884\n",
      "Episode 0, Step Loss: 0.00030167188378982246\n",
      "Episode 0, Step Loss: 0.0006239259382709861\n",
      "Episode 0, Step Loss: 0.00025700527476146817\n",
      "Episode 0, Step Loss: 0.00021323078544810414\n",
      "Episode 0, Step Loss: 0.00017429784929845482\n",
      "Episode 0, Step Loss: 0.00031069378019310534\n",
      "Episode 0, Step Loss: 0.00015208225522655994\n",
      "Episode 0, Step Loss: 0.0006711252499371767\n",
      "Episode 0, Step Loss: 0.0008004765841178596\n",
      "Episode 0, Step Loss: 0.0007429664256051183\n",
      "Episode 0, Step Loss: 7.457775063812733e-05\n",
      "Episode 0, Step Loss: 0.0004088616988155991\n",
      "Episode 0, Step Loss: 0.00023870039149187505\n",
      "Episode 0, Step Loss: 0.0006635861936956644\n",
      "Episode 0, Step Loss: 0.00044601690024137497\n",
      "Episode 0, Step Loss: 0.00041650221101008356\n",
      "Episode 0, Step Loss: 0.00021577435836661607\n",
      "Episode 0, Step Loss: 0.0002697446325328201\n",
      "Episode 0, Step Loss: 0.000342308048857376\n",
      "Episode 0, Step Loss: 0.0005562060396187007\n",
      "Episode 0, Step Loss: 0.0008093832875601947\n",
      "Episode 0, Step Loss: 0.00030647849780507386\n",
      "Episode 0, Step Loss: 0.0003667357377707958\n",
      "Episode 0, Step Loss: 0.00017055544594768435\n",
      "Episode 0, Step Loss: 0.00023165441234596074\n",
      "Episode 0, Step Loss: 0.00022224105487111956\n",
      "Episode 0, Step Loss: 0.0009406600729562342\n",
      "Episode 0, Step Loss: 0.0001862875942606479\n",
      "Episode 0, Step Loss: 0.00030050103669054806\n",
      "Episode 0, Step Loss: 0.000557421357370913\n",
      "Episode 0, Step Loss: 0.00020128196047153324\n",
      "Episode 0, Step Loss: 0.00014514813665300608\n",
      "Episode 0, Step Loss: 0.0001973068283405155\n",
      "Episode 0, Step Loss: 0.0004930515424348414\n",
      "Episode 0, Step Loss: 0.00013648573076352477\n",
      "Episode 0, Step Loss: 0.00025751159409992397\n",
      "Episode 0, Step Loss: 0.00022610704763792455\n",
      "Episode 0, Step Loss: 0.00017193476378452033\n",
      "Episode 0, Step Loss: 0.0003903738979715854\n",
      "Episode 0, Step Loss: 0.0005970865022391081\n",
      "Episode 0, Step Loss: 0.0003982797497883439\n",
      "Episode 0, Step Loss: 0.00024295251932926476\n",
      "Episode 0, Step Loss: 0.000238350810832344\n",
      "Episode 0, Step Loss: 0.00023104842694010586\n",
      "Episode 0, Step Loss: 0.00013673763896804303\n",
      "Episode 0, Step Loss: 0.0001560534437885508\n",
      "Episode 0, Step Loss: 0.00015335161879193038\n",
      "Episode 0, Step Loss: 0.00038447295082733035\n",
      "Episode 0, Step Loss: 0.0008650869713164866\n",
      "Episode 0, Step Loss: 0.00019694759976118803\n",
      "Episode 0, Step Loss: 0.0001546308194519952\n",
      "Episode 0, Step Loss: 0.0004779367009177804\n",
      "Episode 0, Step Loss: 0.00016876589506864548\n",
      "Episode 0, Step Loss: 0.00027828841120935977\n",
      "Episode 0, Step Loss: 0.0002247651864308864\n",
      "Episode 0, Step Loss: 0.00019178682123310864\n",
      "Episode 0, Step Loss: 0.00035969429882243276\n",
      "Episode 0, Step Loss: 0.0002949392655864358\n",
      "Episode 0, Step Loss: 0.00019318360136821866\n",
      "Episode 0, Step Loss: 0.0002543483569752425\n",
      "Episode 0, Step Loss: 0.0003372791688889265\n",
      "Episode 0, Step Loss: 0.0003645670076366514\n",
      "Episode 0, Step Loss: 0.00021221695351414382\n",
      "Episode 0, Step Loss: 0.00039954946259967983\n",
      "Episode 0, Step Loss: 0.0002524855372030288\n",
      "Episode 0, Step Loss: 0.00035716412821784616\n",
      "Episode 0, Step Loss: 0.00015029868518467993\n",
      "Episode 0, Step Loss: 0.00019395277195144445\n",
      "Episode 0, Step Loss: 0.00035802164347842336\n",
      "Episode 0, Step Loss: 0.00036569073563441634\n",
      "Episode 0, Step Loss: 0.0002929650654550642\n",
      "Episode 0, Step Loss: 0.00044092434109188616\n",
      "Episode 0, Step Loss: 0.00014057628868613392\n",
      "Episode 0, Step Loss: 0.0003330639156047255\n",
      "Episode 0, Step Loss: 0.0005314043955877423\n",
      "Episode 0, Step Loss: 0.00014056038344278932\n",
      "Episode 0, Step Loss: 0.0003184806555509567\n",
      "Episode 0, Step Loss: 0.00037226726999506354\n",
      "Episode 0, Step Loss: 0.00020261334429960698\n",
      "Episode 0, Step Loss: 0.00023989449255168438\n",
      "Episode 0, Step Loss: 0.0001927038247231394\n",
      "Episode 0, Step Loss: 0.00012068356591043994\n",
      "Episode 0, Step Loss: 0.00028160124202258885\n",
      "Episode 0, Step Loss: 0.0005377035704441369\n",
      "Episode 0, Step Loss: 0.00043974557775072753\n",
      "Episode 0, Step Loss: 0.0004401253827381879\n",
      "Episode 0, Step Loss: 0.00035927840508520603\n",
      "Episode 0, Step Loss: 0.00024361097894143313\n",
      "Episode 0, Step Loss: 0.00042542387382127345\n",
      "Episode 0, Step Loss: 0.00021120767632964998\n",
      "Episode 0, Step Loss: 0.0001778808218659833\n",
      "Episode 0, Step Loss: 0.000272028089966625\n",
      "Episode 0, Step Loss: 0.0005384071264415979\n",
      "Episode 0, Step Loss: 0.00016294128727167845\n",
      "Episode 0, Step Loss: 0.00036962825106456876\n",
      "Episode 0, Step Loss: 0.00040948594687506557\n",
      "Episode 0, Step Loss: 0.00021693360758945346\n",
      "Episode 0, Step Loss: 0.0002151348744519055\n",
      "Episode 0, Step Loss: 0.0002710772678256035\n",
      "Episode 0, Step Loss: 0.0015288627473637462\n",
      "Episode 0, Step Loss: 0.0003019031719304621\n",
      "Episode 0, Step Loss: 0.00021286799164954573\n",
      "Episode 0, Step Loss: 0.0007133722538128495\n",
      "Episode 0, Step Loss: 0.00018511072266846895\n",
      "Episode 0, Step Loss: 0.0006116772419773042\n",
      "Episode 0, Step Loss: 0.0001887986290967092\n",
      "Episode 0, Step Loss: 0.00019405566854402423\n",
      "Episode 0, Step Loss: 0.0003128022071905434\n",
      "Episode 0, Step Loss: 0.00037711611366830766\n",
      "Episode 0, Step Loss: 0.00011623434693319723\n",
      "Episode 0, Step Loss: 0.00016124175454024225\n",
      "Episode 0, Step Loss: 0.00045860736281611025\n",
      "Episode 0, Step Loss: 0.0011680497555062175\n",
      "Episode 0, Step Loss: 0.00017119827680289745\n",
      "Episode 0, Step Loss: 0.0009025189210660756\n",
      "Episode 0, Step Loss: 0.0001811228576116264\n",
      "Episode 0, Step Loss: 0.0003976445004809648\n",
      "Episode 0, Step Loss: 0.00022659078240394592\n",
      "Episode 0, Step Loss: 0.0002624410262797028\n",
      "Episode 0, Step Loss: 0.00034154870081692934\n",
      "Episode 0, Step Loss: 0.00041748970397748053\n",
      "Episode 0, Step Loss: 0.0004012355348095298\n",
      "Episode 0, Step Loss: 0.0001744575856719166\n",
      "Episode 0, Step Loss: 0.0003266415442340076\n",
      "Episode 0, Step Loss: 0.0002172664098907262\n",
      "Episode 0, Step Loss: 0.0001288501953240484\n",
      "Episode 0, Step Loss: 0.00023880387016106397\n",
      "Episode 0, Step Loss: 0.0009021861478686333\n",
      "Episode 0, Step Loss: 0.0005932927597314119\n",
      "Episode 0, Step Loss: 0.0005542729049921036\n",
      "Episode 0, Step Loss: 0.00043052074033766985\n",
      "Episode 0, Step Loss: 0.0005779160419479012\n",
      "Episode 0, Step Loss: 0.0002538294647820294\n",
      "Episode 0, Step Loss: 0.00020973189384676516\n",
      "Episode 0, Step Loss: 0.00031161640072241426\n",
      "Episode 0, Step Loss: 0.00030940721626393497\n",
      "Episode 0, Step Loss: 0.00040284631540998816\n",
      "Episode 0, Step Loss: 0.000539543863851577\n",
      "Episode 0, Step Loss: 0.0010055247694253922\n",
      "Episode 0, Step Loss: 0.00038712649256922305\n",
      "Episode 0, Step Loss: 0.000235423183767125\n",
      "Episode 0, Step Loss: 0.00022755838290322572\n",
      "Episode 0, Step Loss: 0.0003786414454225451\n",
      "Episode 0, Step Loss: 0.00020953909552190453\n",
      "Episode 0, Step Loss: 0.0001764934422681108\n",
      "Episode 0, Step Loss: 0.0001802182086976245\n",
      "Episode 0, Step Loss: 0.0002349003916606307\n",
      "Episode 0, Step Loss: 0.00038667910848744214\n",
      "Episode 0, Step Loss: 0.00026979163521900773\n",
      "Episode 0, Step Loss: 0.0007993572507984936\n",
      "Episode 0, Step Loss: 0.00012221407087054104\n",
      "Episode 0, Step Loss: 0.0003947667428292334\n",
      "Episode 0, Step Loss: 0.00015175167936831713\n",
      "Episode 0, Step Loss: 0.00019344213069416583\n",
      "Episode 0, Step Loss: 0.0005942296702414751\n",
      "Episode 0, Step Loss: 0.00012975698336958885\n",
      "Episode 0, Step Loss: 0.0002056578960036859\n",
      "Episode 0, Step Loss: 0.00012120892642997205\n",
      "Episode 0, Step Loss: 0.00041217354009859264\n",
      "Episode 0, Step Loss: 0.00044236250687390566\n",
      "Episode 0, Step Loss: 0.0005863223923370242\n",
      "Episode 0, Step Loss: 0.00017207713972311467\n",
      "Episode 0, Step Loss: 0.0003480803279671818\n",
      "Episode 0, Step Loss: 0.00022424134658649564\n",
      "Episode 0, Step Loss: 0.0001877725007943809\n",
      "Episode 0, Step Loss: 0.0003640437498688698\n",
      "Episode 0, Step Loss: 0.0008204210898838937\n",
      "Episode 0, Step Loss: 0.0003305570571683347\n",
      "Episode 0, Step Loss: 0.00020118794054724276\n",
      "Episode 0, Step Loss: 0.0007402626797556877\n",
      "Episode 0, Step Loss: 0.0005693247076123953\n",
      "Episode 0, Step Loss: 0.000596635858528316\n",
      "Episode 0, Step Loss: 0.0005420834640972316\n",
      "Episode 0, Step Loss: 0.0002136299735866487\n",
      "Episode 0, Step Loss: 0.00016603288531769067\n",
      "Episode 0, Step Loss: 0.00027242026408202946\n",
      "Episode 0, Step Loss: 0.0001231364585692063\n",
      "Episode 0, Step Loss: 0.0005346706602722406\n",
      "Episode 0, Step Loss: 0.0002700014738366008\n",
      "Episode 0, Step Loss: 0.0005036256625317037\n",
      "Episode 0, Step Loss: 0.0007054164307191968\n",
      "Episode 0, Step Loss: 0.0004875853192061186\n",
      "Episode 0, Step Loss: 0.0005389052093960345\n",
      "Episode 0, Step Loss: 0.000418764422647655\n",
      "Episode 0, Step Loss: 0.00021470407955348492\n",
      "Episode 0, Step Loss: 0.00020003289682790637\n",
      "Episode 0, Step Loss: 0.00042790514999069273\n",
      "Episode 0, Step Loss: 0.0004609837487805635\n",
      "Episode 0, Step Loss: 0.0005387708079069853\n",
      "Episode 0, Step Loss: 0.00029493478359654546\n",
      "Episode 0, Step Loss: 0.000252052879659459\n",
      "Episode 0, Step Loss: 0.0005340519710443914\n",
      "Episode 0, Step Loss: 0.0001700638676993549\n",
      "Episode 0, Step Loss: 0.00030855555087327957\n",
      "Episode 0, Step Loss: 0.00048625210183672607\n",
      "Episode 0, Step Loss: 0.0003098163288086653\n",
      "Episode 0, Step Loss: 0.00020752045384142548\n",
      "Episode 0, Step Loss: 0.0004132470057811588\n",
      "Episode 0, Step Loss: 0.0005310462438501418\n",
      "Episode 0, Step Loss: 0.00036520068533718586\n",
      "Episode 0, Step Loss: 0.0008914628997445107\n",
      "Episode 0, Step Loss: 0.00025019803433679044\n",
      "Episode 0, Step Loss: 0.00025803595781326294\n",
      "Episode 0, Step Loss: 0.0005951774073764682\n",
      "Episode 0, Step Loss: 0.00018301645468454808\n",
      "Episode 0, Step Loss: 0.0004294608370400965\n",
      "Episode 0, Step Loss: 0.00011711506522260606\n",
      "Episode 0, Step Loss: 0.0003460187290329486\n",
      "Episode 0, Step Loss: 0.0005219271988607943\n",
      "Episode 0, Step Loss: 0.00015819237160030752\n",
      "Episode 0, Step Loss: 0.00012851371138822287\n",
      "Episode 0, Step Loss: 0.0001706184120848775\n",
      "Episode 0, Step Loss: 0.00016775657422840595\n",
      "Episode 0, Step Loss: 0.00019457870803307742\n",
      "Episode 0, Step Loss: 0.00021986252977512777\n",
      "Episode 0, Step Loss: 0.0004405271902214736\n",
      "Episode 0, Step Loss: 0.00026168988551944494\n",
      "Episode 0, Step Loss: 0.00012994515418540686\n",
      "Episode 0, Step Loss: 0.00037074758438393474\n",
      "Episode 0, Step Loss: 0.0003262332465965301\n",
      "Episode 0, Step Loss: 0.00024399685207754374\n",
      "Episode 0, Step Loss: 0.00021689673303626478\n",
      "Episode 0, Step Loss: 0.00020136924285907298\n",
      "Episode 0, Step Loss: 0.00023835990577936172\n",
      "Episode 0, Step Loss: 0.00039354065665975213\n",
      "Episode 0, Step Loss: 0.00038408764521591365\n",
      "Episode 0, Step Loss: 0.0003413937520235777\n",
      "Episode 0, Step Loss: 0.0006317627849057317\n",
      "Episode 0, Step Loss: 0.00023199795396067202\n",
      "Episode 0, Step Loss: 0.0005127444164827466\n",
      "Episode 0, Step Loss: 0.0001990257005672902\n",
      "Episode 0, Step Loss: 0.00024249308626167476\n",
      "Episode 0, Step Loss: 0.0001465597451897338\n",
      "Episode 0, Step Loss: 0.00024279799254145473\n",
      "Episode 0, Step Loss: 0.0003577222814783454\n",
      "Episode 0, Step Loss: 0.0004213450592942536\n",
      "Episode 0, Step Loss: 0.0008379476494155824\n",
      "Episode 0, Step Loss: 0.000497145694680512\n",
      "Episode 0, Step Loss: 0.00032629063935019076\n",
      "Episode 0, Step Loss: 0.00031012098770588636\n",
      "Episode 0, Step Loss: 0.0006261444068513811\n",
      "Episode 0, Step Loss: 0.00024700324865989387\n",
      "Episode 0, Step Loss: 0.0005505781737156212\n",
      "Episode 0, Step Loss: 0.0005619185976684093\n",
      "Episode 0, Step Loss: 0.0002247830998385325\n",
      "Episode 0, Step Loss: 0.0005202050670050085\n",
      "Episode 0, Step Loss: 0.0006317684310488403\n",
      "Episode 0, Step Loss: 0.0006714379996992648\n",
      "Episode 0, Step Loss: 0.00018095057748723775\n",
      "Episode 0, Step Loss: 0.00036662339698523283\n",
      "Episode 0, Step Loss: 0.0010671636555343866\n",
      "Episode 0, Step Loss: 0.00019995449110865593\n",
      "Episode 0, Step Loss: 0.00018828244355972856\n",
      "Episode 0, Step Loss: 0.0010179802775382996\n",
      "Episode 0, Step Loss: 0.0002081827406072989\n",
      "Episode 0, Step Loss: 0.00013223144924268126\n",
      "Episode 0, Step Loss: 0.0006740414537489414\n",
      "Episode 0, Step Loss: 0.00013329688226804137\n",
      "Episode 0, Step Loss: 0.0001888952247099951\n",
      "Episode 0, Step Loss: 0.0001006241436698474\n",
      "Episode 0, Step Loss: 0.00020358186156954616\n",
      "Episode 0, Step Loss: 0.0003901964519172907\n",
      "Episode 0, Step Loss: 0.00020602410950232297\n",
      "Episode 0, Step Loss: 0.0005164650501683354\n",
      "Episode 0, Step Loss: 0.0004757584538310766\n",
      "Episode 0, Step Loss: 0.00046675631892867386\n",
      "Episode 0, Step Loss: 0.00034491930273361504\n",
      "Episode 0, Step Loss: 0.0004783086769748479\n",
      "Episode 0, Step Loss: 0.0006275392952375114\n",
      "Episode 0, Step Loss: 0.00026391708524897695\n",
      "Episode 0, Step Loss: 0.0003200940554961562\n",
      "Episode 0, Step Loss: 0.0005802740924991667\n",
      "Episode 0, Step Loss: 0.000371473201084882\n",
      "Episode 0, Step Loss: 0.000183726500836201\n",
      "Episode 0, Step Loss: 0.0003813564544543624\n",
      "Episode 0, Step Loss: 0.0002205105556640774\n",
      "Episode 0, Step Loss: 0.00036780457594431937\n",
      "Episode 0, Step Loss: 0.0002141901641152799\n",
      "Episode 0, Step Loss: 0.00010148681030841544\n",
      "Episode 0, Step Loss: 0.00046153119183145463\n",
      "Episode 0, Step Loss: 0.0004740190342999995\n",
      "Episode 0, Step Loss: 0.00010565157572273165\n",
      "Episode 0, Step Loss: 0.0003439132706262171\n",
      "Episode 0, Step Loss: 0.0002549832861404866\n",
      "Episode 0, Step Loss: 0.00016106369730550796\n",
      "Episode 0, Step Loss: 0.00031890516402199864\n",
      "Episode 0, Step Loss: 0.0003153412544634193\n",
      "Episode 0, Step Loss: 0.00047845032531768084\n",
      "Episode 0, Step Loss: 0.0003009049978572875\n",
      "Episode 0, Step Loss: 0.00040941248880699277\n",
      "Episode 0, Step Loss: 0.00023877935018390417\n",
      "Episode 0, Step Loss: 0.00048632078687660396\n",
      "Episode 0, Step Loss: 0.0006558927125297487\n",
      "Episode 0, Step Loss: 0.0007663367432542145\n",
      "Episode 0, Step Loss: 0.00012167709792265669\n",
      "Episode 0, Step Loss: 0.0005180543521419168\n",
      "Episode 0, Step Loss: 0.00011446243297541514\n",
      "Episode 0, Step Loss: 0.0004657164972741157\n",
      "Episode 0, Step Loss: 0.0003881676821038127\n",
      "Episode 0, Step Loss: 0.0005704008508473635\n",
      "Episode 0, Step Loss: 0.0003318500821478665\n",
      "Episode 0, Step Loss: 0.0004566251882351935\n",
      "Episode 0, Step Loss: 0.00028921477496623993\n",
      "Episode 0, Step Loss: 0.00030946749029681087\n",
      "Episode 0, Step Loss: 0.0010373340919613838\n",
      "Episode 0, Step Loss: 0.00023094828065950423\n",
      "Episode 0, Step Loss: 0.000321159022860229\n",
      "Episode 0, Step Loss: 0.0001958637440111488\n",
      "Episode 0, Step Loss: 0.00017344174557365477\n",
      "Episode 0, Step Loss: 0.0004937478224746883\n",
      "Episode 0, Step Loss: 0.0007659884286113083\n",
      "Episode 0, Step Loss: 0.00014941082918085158\n",
      "Episode 0, Step Loss: 0.000189303929801099\n",
      "Episode 0, Step Loss: 7.296090188901871e-05\n",
      "Episode 0, Step Loss: 0.0005163998575881124\n",
      "Episode 0, Step Loss: 0.00042858743108808994\n",
      "Episode 0, Step Loss: 0.0005250966059975326\n",
      "Episode 0, Step Loss: 0.00029847558471374214\n",
      "Episode 0, Step Loss: 0.00039201934123411775\n",
      "Episode 0, Step Loss: 0.0004566014977172017\n",
      "Episode 0, Step Loss: 0.00019000637985300273\n",
      "Episode 0, Step Loss: 0.00010447767999721691\n",
      "Episode 0, Step Loss: 0.00033483986044302583\n",
      "Episode 0, Step Loss: 0.0005712660495191813\n",
      "Episode 0, Step Loss: 0.0006310476455837488\n",
      "Episode 0, Step Loss: 0.00028535808087326586\n",
      "Episode 0, Step Loss: 0.0006017411360517144\n",
      "Episode 0, Step Loss: 0.0007482084329240024\n",
      "Episode 0, Step Loss: 0.0002122464356943965\n",
      "Episode 0, Step Loss: 0.0006537626613862813\n",
      "Episode 0, Step Loss: 0.00026046441053040326\n",
      "Episode 0, Step Loss: 0.0003750667383428663\n",
      "Episode 0, Step Loss: 0.0004377873265184462\n",
      "Episode 0, Step Loss: 0.0002241254987893626\n",
      "Episode 0, Step Loss: 0.00019913839059881866\n",
      "Episode 0, Step Loss: 0.0006484582554548979\n",
      "Episode 0, Step Loss: 0.0002626294444780797\n",
      "Episode 0, Step Loss: 0.00025911652483046055\n",
      "Episode 0, Step Loss: 0.00017337649478577077\n",
      "Episode 0, Step Loss: 0.000273752782959491\n",
      "Episode 0, Step Loss: 0.00021377363009378314\n",
      "Episode 0, Step Loss: 0.00044069404248148203\n",
      "Episode 0, Step Loss: 0.00018500504666008055\n",
      "Episode 0, Step Loss: 0.00038658719859085977\n",
      "Episode 0, Step Loss: 0.0003234116011299193\n",
      "Episode 0, Step Loss: 0.0003319529932923615\n",
      "Episode 0, Step Loss: 0.00021011184435337782\n",
      "Episode 0, Step Loss: 0.00038887260598130524\n",
      "Episode 0, Step Loss: 0.00021415477385744452\n",
      "Episode 0, Step Loss: 0.0003110200632363558\n",
      "Episode 0, Step Loss: 0.0008798410417512059\n",
      "Episode 0, Step Loss: 0.00021582198678515851\n",
      "Episode 0, Step Loss: 0.0006302237743511796\n",
      "Episode 0, Step Loss: 0.00035534825292415917\n",
      "Episode 0, Step Loss: 0.00017374350863974541\n",
      "Episode 0, Step Loss: 0.0005101380520500243\n",
      "Episode 0, Step Loss: 0.0006531860562972724\n",
      "Episode 0, Step Loss: 0.00024234491866081953\n",
      "Episode 0, Step Loss: 0.00015715343761257827\n",
      "Episode 0, Step Loss: 0.0004015906888525933\n",
      "Episode 0, Step Loss: 0.00016458815662190318\n",
      "Episode 0, Step Loss: 0.00017413271416444331\n",
      "Episode 0, Step Loss: 0.0005864935228601098\n",
      "Episode 0, Step Loss: 0.00045111856888979673\n",
      "Episode 0, Step Loss: 0.0002658521698322147\n",
      "Episode 0, Step Loss: 0.0003916469868272543\n",
      "Episode 0, Step Loss: 0.00020392141595948488\n",
      "Episode 0, Step Loss: 0.00046755632502026856\n",
      "Episode 0, Step Loss: 0.00018456221732776612\n",
      "Episode 0, Step Loss: 0.0002622045576572418\n",
      "Episode 0, Step Loss: 0.00021324216504581273\n",
      "Episode 0, Step Loss: 0.0003456813283264637\n",
      "Episode 0, Step Loss: 0.00046490872045978904\n",
      "Episode 0, Step Loss: 0.00022829147928860039\n",
      "Episode 0, Step Loss: 0.000236593623412773\n",
      "Episode 0, Step Loss: 0.00034664361737668514\n",
      "Episode 0, Step Loss: 0.0007221638225018978\n",
      "Episode 0, Step Loss: 0.00026433172752149403\n",
      "Episode 0, Step Loss: 0.0008061235421337187\n",
      "Episode 0, Step Loss: 0.0001521059311926365\n",
      "Episode 0, Step Loss: 0.00019416975555941463\n",
      "Episode 0, Step Loss: 0.000435841764556244\n",
      "Episode 0, Step Loss: 0.0003730265889316797\n",
      "Episode 0, Step Loss: 0.00011246518261032179\n",
      "Episode 0, Step Loss: 0.0009845668682828546\n",
      "Episode 0, Step Loss: 0.0002459860988892615\n",
      "Episode 0, Step Loss: 9.477957064518705e-05\n",
      "Episode 0, Step Loss: 0.00130225601606071\n",
      "Episode 0, Step Loss: 0.00015557582082692534\n",
      "Episode 0, Step Loss: 0.0006196189788170159\n",
      "Episode 0, Step Loss: 0.00031298265093937516\n",
      "Episode 0, Step Loss: 0.00019634228374343365\n",
      "Episode 0, Step Loss: 0.0002962149155791849\n",
      "Episode 0, Step Loss: 0.0005201490130275488\n",
      "Episode 0, Step Loss: 0.0006745030404999852\n",
      "Episode 0, Step Loss: 0.0002916613011620939\n",
      "Episode 0, Step Loss: 0.0004233579384163022\n",
      "Episode 0, Step Loss: 0.0003948913945350796\n",
      "Episode 0, Step Loss: 0.0005304149817675352\n",
      "Episode 0, Step Loss: 0.00019577168859541416\n",
      "Episode 0, Step Loss: 0.00039031499181874096\n",
      "Episode 0, Step Loss: 0.00026427095872350037\n",
      "Episode 0, Step Loss: 0.0005000294768251479\n",
      "Episode 0, Step Loss: 0.0003478850412648171\n",
      "Episode 0, Step Loss: 0.00045670103281736374\n",
      "Episode 0, Step Loss: 0.00028491628472693264\n",
      "Episode 0, Step Loss: 0.0001577493385411799\n",
      "Episode 0, Step Loss: 0.00011943995923502371\n",
      "Episode 0, Step Loss: 0.00020437361672520638\n",
      "Episode 0, Step Loss: 0.0005995290703140199\n",
      "Episode 0, Step Loss: 0.0005884924321435392\n",
      "Episode 0, Step Loss: 0.000243721209699288\n",
      "Episode 0, Step Loss: 0.0005822933744639158\n",
      "Episode 0, Step Loss: 0.0004930566647090018\n",
      "Episode 0, Step Loss: 0.0002215399726992473\n",
      "Episode 0, Step Loss: 0.000296880112728104\n",
      "Episode 0, Step Loss: 0.0003127847448922694\n",
      "Episode 0, Step Loss: 0.0005201523308642209\n",
      "Episode 0, Step Loss: 0.0003020201693288982\n",
      "Episode 0, Step Loss: 0.00045831393799744546\n",
      "Episode 0, Step Loss: 0.00037761343992315233\n",
      "Episode 0, Step Loss: 9.801001579035074e-05\n",
      "Episode 0, Step Loss: 0.0006750720785930753\n",
      "Episode 0, Step Loss: 0.00014846527483314276\n",
      "Episode 0, Step Loss: 0.00020239371224306524\n",
      "Episode 0, Step Loss: 0.00023617334954906255\n",
      "Episode 0, Step Loss: 0.00040461597382090986\n",
      "Episode 0, Step Loss: 0.000335725664626807\n",
      "Episode 0, Step Loss: 0.00023866425908636302\n",
      "Episode 0, Step Loss: 0.0001278748532058671\n",
      "Episode 0, Step Loss: 0.00100626889616251\n",
      "Episode 0, Step Loss: 0.0001722360757412389\n",
      "Episode 0, Step Loss: 0.0003772570053115487\n",
      "Episode 0, Step Loss: 0.00038956283242441714\n",
      "Episode 0, Step Loss: 0.0002815901243593544\n",
      "Episode 0, Step Loss: 0.0004869278345722705\n",
      "Episode 0, Step Loss: 0.000252361292950809\n",
      "Episode 0, Step Loss: 0.000639819132629782\n",
      "Episode 0, Step Loss: 0.0002460207906551659\n",
      "Episode 0, Step Loss: 0.00033786255517043173\n",
      "Episode 0, Step Loss: 0.0002130412176484242\n",
      "Episode 0, Step Loss: 0.0006893730023875833\n",
      "Episode 0, Step Loss: 0.0002291227428941056\n",
      "Episode 0, Step Loss: 0.0004757212591357529\n",
      "Episode 0, Step Loss: 0.00048349009011872113\n",
      "Episode 0, Step Loss: 0.000728801591321826\n",
      "Episode 0, Step Loss: 0.00019414084090385586\n",
      "Episode 0, Step Loss: 0.00028471730183809996\n",
      "Episode 0, Step Loss: 0.0001242536964127794\n",
      "Episode 0, Step Loss: 0.00021599323372356594\n",
      "Episode 0, Step Loss: 0.00011445096606621519\n",
      "Episode 0, Step Loss: 0.00021301282686181366\n",
      "Episode 0, Step Loss: 0.00013554916949942708\n",
      "Episode 0, Step Loss: 0.00013800998567603528\n",
      "Episode 0, Step Loss: 0.00019988309941254556\n",
      "Episode 0, Step Loss: 0.00031612403108738363\n",
      "Episode 0, Step Loss: 0.00010376983846072108\n",
      "Episode 0, Step Loss: 0.0005612974055111408\n",
      "Episode 0, Step Loss: 0.00038639362901449203\n",
      "Episode 0, Step Loss: 0.00038458482595160604\n",
      "Episode 0, Step Loss: 0.00020331231644377112\n",
      "Episode 0, Step Loss: 0.0003411179641261697\n",
      "Episode 0, Step Loss: 0.0001718103012535721\n",
      "Episode 0, Step Loss: 0.00030485959723591805\n",
      "Episode 0, Step Loss: 0.000503959774505347\n",
      "Episode 0, Step Loss: 0.001029076986014843\n",
      "Episode 0, Step Loss: 0.0005863416008651257\n",
      "Episode 0, Step Loss: 0.00033582287142053246\n",
      "Episode 0, Step Loss: 0.0005371746374294162\n",
      "Episode 0, Step Loss: 0.0002324538945686072\n",
      "Episode 0, Step Loss: 0.00021308913710527122\n",
      "Episode 0, Step Loss: 0.0006262866081669927\n",
      "Episode 0, Step Loss: 0.0002249130338896066\n",
      "Episode 0, Step Loss: 0.00029601261485368013\n",
      "Episode 0, Step Loss: 0.00016838528972584754\n",
      "Episode 0, Step Loss: 0.00015700449876021594\n",
      "Episode 0, Step Loss: 0.0003606567333918065\n",
      "Episode 0, Step Loss: 0.0003481576277408749\n",
      "Episode 0, Step Loss: 0.00027959406725130975\n",
      "Episode 0, Step Loss: 0.00026433897437527776\n",
      "Episode 0, Step Loss: 0.0009487069328315556\n",
      "Episode 0, Step Loss: 0.00034000942832790315\n",
      "Episode 0, Step Loss: 0.0003615105524659157\n",
      "Episode 0, Step Loss: 0.0002612839743960649\n",
      "Episode 0, Step Loss: 0.0005803165258839726\n",
      "Episode 0, Step Loss: 0.00026071310276165605\n",
      "Episode 0, Step Loss: 0.00044033373706042767\n",
      "Episode 0, Step Loss: 0.00026420524227432907\n",
      "Episode 0, Step Loss: 0.00039739636122249067\n",
      "Episode 0, Step Loss: 0.00027202253113500774\n",
      "Episode 0, Step Loss: 0.000438072020187974\n",
      "Episode 0, Step Loss: 0.00028862172621302307\n",
      "Episode 0, Step Loss: 0.0003406901378184557\n",
      "Episode 0, Step Loss: 0.00029367770184762776\n",
      "Episode 0, Step Loss: 0.00031678241793997586\n",
      "Episode 0, Step Loss: 0.00021948992798570544\n",
      "Episode 0, Step Loss: 0.0007368362275883555\n",
      "Episode 0, Step Loss: 0.0006230633589439094\n",
      "Episode 0, Step Loss: 0.0003000374708790332\n",
      "Episode 0, Step Loss: 0.00031234833295457065\n",
      "Episode 0, Step Loss: 0.00029422968509607017\n",
      "Episode 0, Step Loss: 0.00022193675977177918\n",
      "Episode 0, Step Loss: 0.0003726271679624915\n",
      "Episode 0, Step Loss: 0.00020565146405715495\n",
      "Episode 0, Step Loss: 0.00013121405208949\n",
      "Episode 0, Step Loss: 0.00014626119809690863\n",
      "Episode 0, Step Loss: 0.00013326069165486842\n",
      "Episode 0, Step Loss: 0.0005170590593479574\n",
      "Episode 0, Step Loss: 0.0004537220811471343\n",
      "Episode 0, Step Loss: 0.00016183374100364745\n",
      "Episode 0, Step Loss: 0.0001850101543823257\n",
      "Episode 0, Step Loss: 0.00029418503982014954\n",
      "Episode 0, Step Loss: 0.00016920434427447617\n",
      "Episode 0, Step Loss: 0.00023243844043463469\n",
      "Episode 0, Step Loss: 0.0001308870705543086\n",
      "Episode 0, Step Loss: 0.0003714404010679573\n",
      "Episode 0, Step Loss: 0.0005817574565298855\n",
      "Episode 0, Step Loss: 0.00021889439085498452\n",
      "Episode 0, Step Loss: 0.0005826061824336648\n",
      "Episode 0, Step Loss: 0.00025892205303534865\n",
      "Episode 0, Step Loss: 0.00019386832718737423\n",
      "Episode 0, Step Loss: 0.00031401796150021255\n",
      "Episode 0, Step Loss: 0.00021669715351890773\n",
      "Episode 0, Step Loss: 0.0003610370331443846\n",
      "Episode 0, Step Loss: 0.00014098033716436476\n",
      "Episode 0, Step Loss: 0.00018089002696797252\n",
      "Episode 0, Step Loss: 0.00017661972378846258\n",
      "Episode 0, Step Loss: 0.0005306974053382874\n",
      "Episode 0, Step Loss: 0.00011601029109442607\n",
      "Episode 0, Step Loss: 0.00038845324888825417\n",
      "Episode 0, Step Loss: 0.0004495400353334844\n",
      "Episode 0, Step Loss: 0.0001187263333122246\n",
      "Episode 0, Step Loss: 0.0004063521628268063\n",
      "Episode 0, Step Loss: 0.0002893018536269665\n",
      "Episode 0, Step Loss: 0.00027761008823290467\n",
      "Episode 0, Step Loss: 0.00044158700620755553\n",
      "Episode 0, Step Loss: 0.0004315250553190708\n",
      "Episode 0, Step Loss: 0.00012151493865530938\n",
      "Episode 0, Step Loss: 0.0001548013387946412\n",
      "Episode 0, Step Loss: 0.0005856414209119976\n",
      "Episode 0, Step Loss: 0.00029658523271791637\n",
      "Episode 0, Step Loss: 0.00033148739021271467\n",
      "Episode 0, Step Loss: 0.00016930588753893971\n",
      "Episode 0, Step Loss: 0.0003254558832850307\n",
      "Episode 0, Step Loss: 0.00022638140944764018\n",
      "Episode 0, Step Loss: 0.0005365386605262756\n",
      "Episode 0, Step Loss: 0.0001424729562131688\n",
      "Episode 0, Step Loss: 0.0005162444431334734\n",
      "Episode 0, Step Loss: 0.00017799789202399552\n",
      "Episode 0, Step Loss: 0.00019718764815479517\n",
      "Episode 0, Step Loss: 0.0002352759474888444\n",
      "Episode 0, Step Loss: 0.0004448634572327137\n",
      "Episode 0, Step Loss: 0.00026958578382618725\n",
      "Episode 0, Step Loss: 0.0001672091893851757\n",
      "Episode 0, Step Loss: 0.00017642865714151412\n",
      "Episode 0, Step Loss: 0.0002810746955219656\n",
      "Episode 0, Step Loss: 0.0001670005585765466\n",
      "Episode 0, Step Loss: 0.00016041121853049845\n",
      "Episode 0, Step Loss: 0.0002173772663809359\n",
      "Episode 0, Step Loss: 0.00025414119591005147\n",
      "Episode 0, Step Loss: 0.00024071623920463026\n",
      "Episode 0, Step Loss: 0.00011537712998688221\n",
      "Episode 0, Step Loss: 0.0002700794721022248\n",
      "Episode 0, Step Loss: 0.00016438619059044868\n",
      "Episode 0, Step Loss: 0.00017649611982051283\n",
      "Episode 0, Step Loss: 0.0006853684317320585\n",
      "Episode 0, Step Loss: 0.0006461450248025358\n",
      "Episode 0, Step Loss: 0.000199895424884744\n",
      "Episode 0, Step Loss: 0.0003527262597344816\n",
      "Episode 0, Step Loss: 0.0006613265722990036\n",
      "Episode 0, Step Loss: 0.00021246531105134636\n",
      "Episode 0, Step Loss: 0.0002495715452823788\n",
      "Episode 0, Step Loss: 0.00048554124077782035\n",
      "Episode 0, Step Loss: 0.0002845107519533485\n",
      "Episode 0, Step Loss: 0.0008746875682845712\n",
      "Episode 0, Step Loss: 0.00033168066875077784\n",
      "Episode 0, Step Loss: 0.0001625677541596815\n",
      "Episode 0, Step Loss: 0.0002360296930419281\n",
      "Episode 0, Step Loss: 0.000445926416432485\n",
      "Episode 0, Step Loss: 0.00027195701841264963\n",
      "Episode 0, Step Loss: 0.0002472751366440207\n",
      "Episode 0, Step Loss: 0.00019450047693680972\n",
      "Episode 0, Step Loss: 0.0002582099987193942\n",
      "Episode 0, Step Loss: 0.0003703917609527707\n",
      "Episode 0, Step Loss: 0.00016167772992048413\n",
      "Episode 0, Step Loss: 0.00016399532614741474\n",
      "Episode 0, Step Loss: 0.00018646342505235225\n",
      "Episode 0, Step Loss: 0.0003465439658612013\n",
      "Episode 0, Step Loss: 0.0003044561017304659\n",
      "Episode 0, Step Loss: 0.0007418983150273561\n",
      "Episode 0, Step Loss: 0.0002932921051979065\n",
      "Episode 0, Step Loss: 0.00016420998144894838\n",
      "Episode 0, Step Loss: 0.0002256776933791116\n",
      "Episode 0, Step Loss: 0.0002136050898116082\n",
      "Episode 0, Step Loss: 0.00020538235548883677\n",
      "Episode 0, Step Loss: 0.00041200313717126846\n",
      "Episode 0, Step Loss: 0.0001492664305260405\n",
      "Episode 0, Step Loss: 0.0005178843857720494\n",
      "Episode 0, Step Loss: 0.000161518226377666\n",
      "Episode 0, Step Loss: 0.0004090939764864743\n",
      "Episode 0, Step Loss: 0.0001525732659501955\n",
      "Episode 0, Step Loss: 0.0002934932999778539\n",
      "Episode 0, Step Loss: 0.00021041785657871515\n",
      "Episode 0, Step Loss: 0.0004922837251797318\n",
      "Episode 0, Step Loss: 0.0006865317118354142\n",
      "Episode 0, Step Loss: 0.0002582680608611554\n",
      "Episode 0, Step Loss: 0.0002797419438138604\n",
      "Episode 0, Step Loss: 0.00032752889092080295\n",
      "Episode 0, Step Loss: 0.00043197741615585983\n",
      "Episode 0, Step Loss: 0.00035108262090943754\n",
      "Episode 0, Step Loss: 0.00013191190373618156\n",
      "Episode 0, Step Loss: 0.00014979738625697792\n",
      "Episode 0, Step Loss: 0.0009520684252493083\n",
      "Episode 0, Step Loss: 0.0007121614180505276\n",
      "Episode 0, Step Loss: 0.00025687614106573164\n",
      "Episode 0, Step Loss: 0.000424634461523965\n",
      "Episode 0, Step Loss: 0.0005467724986374378\n",
      "Episode 0, Step Loss: 0.00026568968314677477\n",
      "Episode 0, Step Loss: 0.0002435087226331234\n",
      "Episode 0, Step Loss: 0.0003339471877552569\n",
      "Episode 0, Step Loss: 0.00016809171938803047\n",
      "Episode 0, Step Loss: 0.00016984013200271875\n",
      "Episode 0, Step Loss: 0.0002415781345916912\n",
      "Episode 0, Step Loss: 0.0006397603428922594\n",
      "Episode 0, Step Loss: 0.0009155913721770048\n",
      "Episode 0, Step Loss: 0.00021876534447073936\n",
      "Episode 0, Step Loss: 0.0003467830829322338\n",
      "Episode 0, Step Loss: 0.0006794369546696544\n",
      "Episode 0, Step Loss: 0.00016908910765778273\n",
      "Episode 0, Step Loss: 7.947645644890144e-05\n",
      "Episode 0, Step Loss: 0.00014814494352322072\n",
      "Episode 0, Step Loss: 0.00022314996749628335\n",
      "Episode 0, Step Loss: 0.00012137966405134648\n",
      "Episode 0, Step Loss: 0.0001212497300002724\n",
      "Episode 0, Step Loss: 9.699618385639042e-05\n",
      "Episode 0, Step Loss: 0.00019064261869061738\n",
      "Episode 0, Step Loss: 0.0003161285421811044\n",
      "Episode 0, Step Loss: 0.0005089403712190688\n",
      "Episode 0, Step Loss: 0.0002199990558438003\n",
      "Episode 0, Step Loss: 0.0009869660716503859\n",
      "Episode 0, Step Loss: 0.000358593970304355\n",
      "Episode 0, Step Loss: 0.00018571983673609793\n",
      "Episode 0, Step Loss: 0.0003204294480383396\n",
      "Episode 0, Step Loss: 0.0009889091597869992\n",
      "Episode 0, Step Loss: 0.00036170490784570575\n",
      "Episode 0, Step Loss: 0.00028255939832888544\n",
      "Episode 0, Step Loss: 0.0005267906817607582\n",
      "Episode 0, Step Loss: 0.00037385066389106214\n",
      "Episode 0, Step Loss: 0.00023670989321544766\n",
      "Episode 0, Step Loss: 0.00020939105888828635\n",
      "Episode 0, Step Loss: 0.0001893971930257976\n",
      "Episode 0, Step Loss: 0.00035857129842042923\n",
      "Episode 0, Step Loss: 0.0003517476434353739\n",
      "Episode 0, Step Loss: 0.00023758782481309026\n",
      "Episode 0, Step Loss: 0.00040173332672566175\n",
      "Episode 0, Step Loss: 0.0006014961400069296\n",
      "Episode 0, Step Loss: 0.0004306061891838908\n",
      "Episode 0, Step Loss: 0.0007064871606417\n",
      "Episode 0, Step Loss: 0.0003026347258128226\n",
      "Episode 0, Step Loss: 0.0004389586392790079\n",
      "Episode 0, Step Loss: 0.00023241410963237286\n",
      "Episode 0, Step Loss: 0.00021051890507806093\n",
      "Episode 0, Step Loss: 0.0004753308603540063\n",
      "Episode 0, Step Loss: 0.00034943860373459756\n",
      "Episode 0, Step Loss: 0.00030071212677285075\n",
      "Episode 0, Step Loss: 0.0003266844432801008\n",
      "Episode 0, Step Loss: 0.00031359607237391174\n",
      "Episode 0, Step Loss: 0.00025193399051204324\n",
      "Episode 0, Step Loss: 0.00012998869351577014\n",
      "Episode 0, Step Loss: 0.00012733726180158556\n",
      "Episode 0, Step Loss: 0.0001519407087471336\n",
      "Episode 0, Step Loss: 0.0002573316160123795\n",
      "Episode 0, Step Loss: 0.0003208205453120172\n",
      "Episode 0, Step Loss: 0.0010057269828394055\n",
      "Episode 0, Step Loss: 0.00022445754439104348\n",
      "Episode 0, Step Loss: 0.0008553648949600756\n",
      "Episode 0, Step Loss: 0.00023431648151017725\n",
      "Episode 0, Step Loss: 0.00025736901443451643\n",
      "Episode 0, Step Loss: 0.003739546751603484\n",
      "Episode 0, Step Loss: 0.0007609518361277878\n",
      "Episode 0, Step Loss: 0.00031519008916802704\n",
      "Episode 0, Step Loss: 0.000345288950484246\n",
      "Episode 0, Step Loss: 0.0003483433974906802\n",
      "Episode 0, Step Loss: 0.0004989235894754529\n",
      "Episode 0, Step Loss: 0.00017093602218665183\n",
      "Episode 0, Step Loss: 0.0006344434805214405\n",
      "Episode 0, Step Loss: 0.0003048211510758847\n",
      "Episode 0, Step Loss: 0.0006728352745994925\n",
      "Episode 0, Step Loss: 0.0004022093489766121\n",
      "Episode 0, Step Loss: 0.000412164896260947\n",
      "Episode 0, Step Loss: 0.00020797544857487082\n",
      "Episode 0, Step Loss: 0.0002266092342324555\n",
      "Episode 0, Step Loss: 0.0007840209873393178\n",
      "Episode 0, Step Loss: 0.0001567656290717423\n",
      "Episode 0, Step Loss: 0.0003031655214726925\n",
      "Episode 0, Step Loss: 0.0003979050670750439\n",
      "Episode 0, Step Loss: 0.00022023091150913388\n",
      "Episode 0, Step Loss: 0.0006747172446921468\n",
      "Episode 0, Step Loss: 0.00036324464599601924\n",
      "Episode 0, Step Loss: 0.00020925188437104225\n",
      "Episode 0, Step Loss: 0.0001395954459439963\n",
      "Episode 0, Step Loss: 0.0003694075858220458\n",
      "Episode 0, Step Loss: 0.00016102871450129896\n",
      "Episode 0, Step Loss: 0.001130994176492095\n",
      "Episode 0, Step Loss: 0.0003289596061222255\n",
      "Episode 0, Step Loss: 0.00024448620388284326\n",
      "Episode 0, Step Loss: 0.0005056189256720245\n",
      "Episode 0, Step Loss: 0.00022780164727009833\n",
      "Episode 0, Step Loss: 0.0006255932967178524\n",
      "Episode 0, Step Loss: 0.00022112653823569417\n",
      "Episode 0, Step Loss: 0.0006397922988981009\n",
      "Episode 0, Step Loss: 0.0006053567049093544\n",
      "Episode 0, Step Loss: 0.0001283989695366472\n",
      "Episode 0, Step Loss: 0.0002666667860466987\n",
      "Episode 0, Step Loss: 0.0004253238730598241\n",
      "Episode 0, Step Loss: 0.0011940998956561089\n",
      "Episode 0, Step Loss: 0.0002206049393862486\n",
      "Episode 0, Step Loss: 0.00028337957337498665\n",
      "Episode 0, Step Loss: 0.0004974556504748762\n",
      "Episode 0, Step Loss: 0.00019820945453830063\n",
      "Episode 0, Step Loss: 0.0003409269847907126\n",
      "Episode 0, Step Loss: 0.0009677450289018452\n",
      "Episode 0, Step Loss: 0.00015721845556981862\n",
      "Episode 0, Step Loss: 0.0005867267609573901\n",
      "Episode 0, Step Loss: 0.0004763500182889402\n",
      "Episode 0, Step Loss: 0.00022384086332749575\n",
      "Episode 0, Step Loss: 0.0003896031412295997\n",
      "Episode 0, Step Loss: 0.0003864290192723274\n",
      "Episode 0, Step Loss: 0.00032431870931759477\n",
      "Episode 0, Step Loss: 0.00029904392431490123\n",
      "Episode 0, Step Loss: 0.0003511861723382026\n",
      "Episode 0, Step Loss: 0.0002531894133426249\n",
      "Episode 0, Step Loss: 0.0004401008191052824\n",
      "Episode 0, Step Loss: 0.00022271298803389072\n",
      "Episode 0, Step Loss: 0.0005511139752343297\n",
      "Episode 0, Step Loss: 0.0003482079191599041\n",
      "Episode 0, Step Loss: 0.00031375110847875476\n",
      "Episode 0, Step Loss: 0.0005926279118284583\n",
      "Episode 0, Step Loss: 0.0005599330179393291\n",
      "Episode 0, Step Loss: 0.00035076370113529265\n",
      "Episode 0, Step Loss: 0.00031432692776434124\n",
      "Episode 0, Step Loss: 0.0003244345134589821\n",
      "Episode 0, Step Loss: 0.0005528424517251551\n",
      "Episode 0, Step Loss: 0.00043733848724514246\n",
      "Episode 0, Step Loss: 0.0005953666986897588\n",
      "Episode 0, Step Loss: 0.0001465580571675673\n",
      "Episode 0, Step Loss: 0.00017866016423795372\n",
      "Episode 0, Step Loss: 0.00017002753156702965\n",
      "Episode 0, Step Loss: 0.00030411785701289773\n",
      "Episode 0, Step Loss: 0.0009028743952512741\n",
      "Episode 0, Step Loss: 0.0007432149141095579\n",
      "Episode 0, Step Loss: 0.00036729127168655396\n",
      "Episode 0, Step Loss: 0.0002884584537241608\n",
      "Episode 0, Step Loss: 0.00023122639686334878\n",
      "Episode 0, Step Loss: 0.0002776603796519339\n",
      "Episode 0, Step Loss: 0.000509292003698647\n",
      "Episode 0, Step Loss: 0.0007090892759151757\n",
      "Episode 0, Step Loss: 0.0006078301230445504\n",
      "Episode 0, Step Loss: 0.0004928433336317539\n",
      "Episode 0, Step Loss: 0.0002643069892656058\n",
      "Episode 0, Step Loss: 0.00024142694019246846\n",
      "Episode 0, Step Loss: 0.00017527466116007417\n",
      "Episode 0, Step Loss: 0.00022114923922345042\n",
      "Episode 0, Step Loss: 0.00017023705004248768\n",
      "Episode 0, Step Loss: 0.00031565732206217945\n",
      "Episode 0, Step Loss: 0.00035270239459350705\n",
      "Episode 0, Step Loss: 0.0002989410131704062\n",
      "Episode 0, Step Loss: 0.0006075348937883973\n",
      "Episode 0, Step Loss: 0.0005332661094143987\n",
      "Episode 0, Step Loss: 0.0002987513435073197\n",
      "Episode 0, Step Loss: 0.00018305311095900834\n",
      "Episode 0, Step Loss: 0.00018725120753515512\n",
      "Episode 0, Step Loss: 0.0003320082323625684\n",
      "Episode 0, Step Loss: 0.00038519129157066345\n",
      "Episode 0, Step Loss: 0.0002610110677778721\n",
      "Episode 0, Step Loss: 0.00033166739740408957\n",
      "Episode 0, Step Loss: 0.00016762648010626435\n",
      "Episode 0, Step Loss: 0.00010915605525951833\n",
      "Episode 0, Step Loss: 0.00010118536010850221\n",
      "Episode 0, Step Loss: 0.0002230737591162324\n",
      "Episode 0, Step Loss: 0.00025519932387396693\n",
      "Episode 0, Step Loss: 0.00025986769469454885\n",
      "Episode 0, Step Loss: 0.0007045847014524043\n",
      "Episode 0, Step Loss: 0.00015914958203211427\n",
      "Episode 0, Step Loss: 0.00016480399062857032\n",
      "Episode 0, Step Loss: 0.00013153963664080948\n",
      "Episode 0, Step Loss: 0.0006126366788521409\n",
      "Episode 0, Step Loss: 0.0001072484374162741\n",
      "Episode 0, Step Loss: 0.00034568062983453274\n",
      "Episode 0, Step Loss: 0.00035387836396694183\n",
      "Episode 0, Step Loss: 0.00023212422092910856\n",
      "Episode 0, Step Loss: 0.00020500908431131393\n",
      "Episode 0, Step Loss: 0.00043627587729133666\n",
      "Episode 0, Step Loss: 0.0001957856584340334\n",
      "Episode 0, Step Loss: 0.0005548628396354616\n",
      "Episode 0, Step Loss: 0.00012395679368637502\n",
      "Episode 0, Step Loss: 0.00045894747017882764\n",
      "Episode 0, Step Loss: 0.00046542397467419505\n",
      "Episode 0, Step Loss: 0.0007412216509692371\n",
      "Episode 0, Step Loss: 0.0005368962301872671\n",
      "Episode 0, Step Loss: 0.000274765829090029\n",
      "Episode 0, Step Loss: 0.00045996138942427933\n",
      "Episode 0, Step Loss: 0.00030771829187870026\n",
      "Episode 0, Step Loss: 0.00023211189545691013\n",
      "Episode 0, Step Loss: 0.0007095595356076956\n",
      "Episode 0, Step Loss: 0.0003541311016306281\n",
      "Episode 0, Step Loss: 0.00044495699694380164\n",
      "Episode 0, Step Loss: 0.000593687582295388\n",
      "Episode 0, Step Loss: 0.0006931790267117321\n",
      "Episode 0, Step Loss: 0.0003825756430160254\n",
      "Episode 0, Step Loss: 0.0008101318962872028\n",
      "Episode 0, Step Loss: 0.00021728970750700682\n",
      "Episode 0, Step Loss: 0.00022905005607753992\n",
      "Episode 0, Step Loss: 0.00029482762329280376\n",
      "Episode 0, Step Loss: 0.0003638327762018889\n",
      "Episode 0, Step Loss: 0.0006810594350099564\n",
      "Episode 0, Step Loss: 0.00034877151483669877\n",
      "Episode 0, Step Loss: 0.00022360129514709115\n",
      "Episode 0, Step Loss: 0.0001988222065847367\n",
      "Episode 0, Step Loss: 0.0001987125724554062\n",
      "Episode 0, Step Loss: 0.00035378136090002954\n",
      "Episode 0, Step Loss: 0.00034375101677142084\n",
      "Episode 0, Step Loss: 0.0003912044339813292\n",
      "Episode 0, Step Loss: 7.506240217480808e-05\n",
      "Episode 0, Step Loss: 0.00013066880637779832\n",
      "Episode 0, Step Loss: 0.000798594206571579\n",
      "Episode 0, Step Loss: 0.00025286251911893487\n",
      "Episode 0, Step Loss: 0.0011070120381191373\n",
      "Episode 0, Step Loss: 0.0003218955534975976\n",
      "Episode 0, Step Loss: 0.0004440760239958763\n",
      "Episode 0, Step Loss: 0.0002643862972036004\n",
      "Episode 0, Step Loss: 0.0003775611985474825\n",
      "Episode 0, Step Loss: 0.0011125075398012996\n",
      "Episode 0, Step Loss: 0.0003307005390524864\n",
      "Episode 0, Step Loss: 0.0003868629573844373\n",
      "Episode 0, Step Loss: 0.0003093551204074174\n",
      "Episode 0, Step Loss: 0.00023270174278877676\n",
      "Episode 0, Step Loss: 0.0007469138945452869\n",
      "Episode 0, Step Loss: 0.000741831143386662\n",
      "Episode 0, Step Loss: 0.00016775508993305266\n",
      "Episode 0, Step Loss: 0.0003886684717144817\n",
      "Episode 0, Step Loss: 0.000138784758746624\n",
      "Episode 0, Step Loss: 0.00015815990627743304\n",
      "Episode 0, Step Loss: 0.0004221987328492105\n",
      "Episode 0, Step Loss: 0.0006214813329279423\n",
      "Episode 0, Step Loss: 0.00022430875105783343\n",
      "Episode 0, Step Loss: 0.0004117126518394798\n",
      "Episode 0, Step Loss: 0.0003265312116127461\n",
      "Episode 0, Step Loss: 0.00022749483468942344\n",
      "Episode 0, Step Loss: 0.00047744426410645247\n",
      "Episode 0, Step Loss: 0.0008051243494264781\n",
      "Episode 0, Step Loss: 0.00034238092484883964\n",
      "Episode 0, Step Loss: 0.0004194245848339051\n",
      "Episode 0, Step Loss: 0.00037864482146687806\n",
      "Episode 0, Step Loss: 0.000219453388126567\n",
      "Episode 0, Step Loss: 0.0006123651401139796\n",
      "Episode 0, Step Loss: 0.0004968037828803062\n",
      "Episode 0, Step Loss: 0.0007971509476192296\n",
      "Episode 0, Step Loss: 0.0004245707532390952\n",
      "Episode 0, Step Loss: 0.00020392637816257775\n",
      "Episode 0, Step Loss: 0.0004079141654074192\n",
      "Episode 0, Step Loss: 0.0004300181462895125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the train_dqn function with the gaemodel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dqn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgae_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgaemodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(gae_model, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m next_state \u001b[38;5;241m=\u001b[39m robot\u001b[38;5;241m.\u001b[39msample_grid()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     next_state_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mgae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     next_state_embedding \u001b[38;5;241m=\u001b[39m next_state_embedding\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m done \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mGAE.forward\u001b[0;34m(self, node_features, edge_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_78cval97.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[1;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    167\u001b[0m                 alpha_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    168\u001b[0m                 alpha_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    173\u001b[0m             )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py:409\u001b[0m, in \u001b[0;36mGATConv.edge_update\u001b[0;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    406\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m alpha_edge\n\u001b[1;32m    408\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[0;32m--> 409\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch_geometric/utils/_softmax.py:78\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[1;32m     76\u001b[0m     out \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m-\u001b[39m src_max\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[1;32m     77\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 78\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n\u001b[1;32m     79\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m out_sum\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.8/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the train_dqn function with the gaemodel\n",
    "dqn_model = train_dqn(gae_model=gaemodel, num_episodes=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
